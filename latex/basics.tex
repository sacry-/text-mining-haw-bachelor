\epigraph{\emph{
  ``If I have seen further it is by standing on the shoulders of giants.''
}}{ Isaac Newton }

The goal of this section is to give some intuition and the necessary theoretical background for the following chapters. The areas where clustering problems arise are huge. It provides solutions to problems like market segmentation, classification, document organization or indexing.\\
Firstly we will have a look at the definition of clustering and summarization. How they are related and the variety of possibilities this imposes.
Secondly the vector space model (VSM) is introduced. It contains all information about how to represent documents in a vectorized form. Of special interest are enhanced models which reduce the dimensionality of documents by singular value decomposition (SVD).
Thirdly traditional clustering algorithms from the hierarchical (Ward, Birch) and partitional (K-Means, Mean-Shift) family will be presented.
Closely related are the generative models. These methods can be used as a kind of clustering algorithm and are highly useful in several steps of traditional clustering. They can be used as dimensionality reduction techniques as well.
Lastly some quality measures of clusters based on internal measures (without ground truth labels) and external measures (with explicit labelling of the ground truth) are explained.


\section{Clustering and Summarization}
  
  \textbf{Clustering} as defined by \cite{ClusterAlgoSurveyIBM} is finding groups of similar objects in the data with a defined similarity function between objects. The granularity of the features can vary:

  \begin{itemize}
    \item \emph{Sentence based} - A document d is split into sentences so clustering reveals the most coherent groups of sentences that are closely related.
    \item \emph{Collection of documents} - A collection of documents d (corpora) is grouped to get groups of documents that are closely related
    \item \emph{Stream of documents} - The same as clustering copora with the constraint that over time the size of documents grow.
  \end{itemize}

  The following is an illustration of a typical cluster result. The larger points correspond to cluster centers while the smaller points correspond to individual documents adhering to the cluster centers colors. 

  \begin{figure}[h!]
    \centering
      \includegraphics[width=0.7\textwidth]{clustering_intro.png}
      \caption{"K-means visualisation with applied dimensionality reduction"}
      \label{clustering_intro}
  \end{figure}

  Document clustering on large corpora can be seen as a summarization of the underlying concepts. The representation of documents as feature vectors is described with the vector space model in the next section. Data clustering is a computationally expensive \emph{NP-hard} problem. That means there is currently no efficient way to group objectcs in an optimal way. Therefore heuristics are applied such that algorithms converge at a  \emph{local minimum}. Theoretically a local minimum can vary vastly compared to a global minimum, in practice however a close to 80\% solution seems reasonable.\\
  \textbf{Automatic text summarization} on the other hand, is the process of reducing textual content to the most important concepts in a readable, formatted form to the user \cite{SumEvaluation2001}.\\
  This results in a few possibilities where clustering works great as a preprocessing step for summarization.
  
  \begin{description}
    \item[First] Clustering groups that have a \emph{higher density of information} resulting in a grouped input for summarizers.
    \item[Second] Grouping the \emph{latent topics} accross and within documents to create a meta concept of closely related documents
    \item[Third] Classify documents into \emph{categories} in a semi supervised way to construct hierarchies of relationships
    \item[Fourth] Finding \emph{outliers} that will not highly contribute to the summarization
  \end{description}

  Aside clustering itself can be seen as a summarization as well. Clustering can lead to well formed topical browsers where users can interact with a graphical user interface to browse topics in a more coherent and semantic way see \cite{Carrot2Search2003}.

  \paragraph{Supervision}

    As opposed to unsupervised learning strategies such as clustering, supervised learning classifies some input based on a provided ground truth. That is for an input \emph{x} there are labels \emph{y} that describe the class they are in. Supervision can be done by explicitly classifying the documents before the clustering. The input is then split into \emph{n} classes. Then each class can be individually clustered. Often however this is no option. We need to manually label all documents. This can be time consuming and error prone. Often several labellers are needed to crossvalidate human bias.
    With this in mind there are two options on how to label unseen or new data:

      \begin{itemize}
        \item Use a supervised classification algorithm to automatically label unlabelled data. A prerequisite is to have a labelled training set and to have a lot of data. To name a few candidates: \emph{Multinominal/Gaussian) Naive Bayes (NB)}, \emph{Multivariate Logistic/Linear Regression}, \emph{Neural Networks (ANN)}, \emph{Support Vector Machines (SVM)} or \emph{Random Forests (RF)}.
        \item Use an unsupervised clustering algorithm to automatically label unlabelled data. This can be done by first forming clusters and then merging the nearest clusters until k distinct categories remain. Usually the merging criterion can be controlled by some threshold and high variance documents are sorted out into an outlier cluster.
      \end{itemize}

    Usually by clustering we mean automatic detection of the grund truths. Often this is to shallow and does not lead to labels with a high confidence. In the domain of document clustering all information that provide some context are critical and should be used.

\newpage{}
\section{Vector Space Model (VSM)}
  
  \paragraph{}
    The vector space model is directly derived from the vector space subject to linear algebra. If we talk about vector space we often refer to the euclidean vector space where the dimensions are typically up to 3 dimensions. All dimensions higher than 3 are hard to imagine. From the view of linear algebra the vector space consists of linear combinations that are solved by $Ax = b$ by some matrix decomposition step. In the context of document clustering the vector spaces typically far exceed 3 dimensions up to $\mathbf{R}^{n}$. For a proper introduction to linear algebra see \cite{Strang2009}.

  \begin{figure}[h!]
    \centering
      \includegraphics[width=0.7\textwidth]{vsm.png}
      \caption{"Vector space model"}
      \label{vsm_pic}
  \end{figure}

  \paragraph{}
    The vector space model in the text domain has the meaning that each word is a component of a document. If a document has 100 distinct words, the resulting document vector is in 100th dimensional space. If a second document has 50 distinct words, independant of the first document, both vectors are now in 150th dimensional space. That is every new word will be concatenated to the existing document sets. This is the ``bag of words'' model which is detailed in the next section.

  \subsection{Notation}
    Before moving on we have to denote some notations and definitions. 
    A corpus \emph{C} is defined as a collection of \emph{m} documents

    \begin{equation}
      C = \{d_1, d_2 \: .. \: d_m\}
    \end{equation}

    A document \emph{d} contains words \emph{w} such that
    
    \begin{equation}
      d = \{w_1, w_2 \: .. \: w_i\}
    \end{equation}
    
    Where \emph{i} is \emph{|d| (length of document d)}. Note that each document is not a text but a sequence of words. That is the result of preprocessing where texts are tokenized to individual word tokens. Often special characters are filtered out as well.\\
    A dictionary \emph{D} contains all the distinct words from each document.

    \begin{equation}
      D = \{w_1, w_2 \: .. \: w_n\}
    \end{equation}

  \subsection{Bag of words}
    The bag of words assumption says that a corpus can be represented as a count or word occurence matrix. That means \emph{m} documents form a subspace in an \emph{m x n} matrix where \emph{m} denotes the corpus size and \emph{n} the dictionary of the words. Typically the assumption is binary, that is an occurence of word \emph{i} in a document \emph{j} is set to 1 else set to 0. One of the general assumptions is directly derived from Bayesian inference that words are indenpendant of each other. That is, there are no relations between a given word and the following word in a document. In reality this is not true, but without this assumption it cannot hold true that a bag of words is a suitable representation of text data. In real world examples this typically no problem.

    \begin{table}[h!]
      \centering
      \begin{tabular}{c|c|c|c}
        \multicolumn{1}{r|}{} & \multicolumn{3}{c}{Words} \\
        \cline{1-4}
        Documents &   politics &   corruption &  policy  \\
        \hline
        document1 &    1 (2)   &     1 (1)    &   0 (0)  \\
        document2 &    1 (4)   &     0 (0)    &   1 (2)  \\
        document3 &    1 (1)   &     1 (6)    &   0 (0)  \\
      \end{tabular}\\
      \caption{"Document term matrix, the numbers in brackets is the actual count"}
    \end{table}

    Normally we have a lower \emph{m} and a much higher \emph{n} resulting in highly sparse vectors with a lot of zeros, typically 99\%. We can also conclude that the order of words is not kept. That means highly correlating words like \emph{New} and \emph{York} are not accounted for. In this case they can actually refer to the verb \emph{new} and \emph{York} as a city in \emph{Great Britain}. A bigram e.g. \emph{(New, York)} would capture the concept \emph{New York}. Bigrams, trigrams or generally ngrams are not taken into account. Ngrams transform a sequence of words by \emph{n} such that 

      \begin{equation}
        f(n = 2, \{w_1, w_2, w_3\} \in d) \to \{(w_1, w_2),(w_2,w_3)\}
      \end{equation}

    Adding ngrams to a document term matrix can greatly enhance similiarity between two documents. With single words and ngrams the memory requirement is \emph{n+(n-1)}. If the data is already extremely sparse this will not help much but can increase the semantic effect on documents that share similar word combinations. There are other much more complex models e.g. \emph{noun phrases} or \emph{named entities} that can grasp this intuition as well.

    The document term matrix can be enhanced by taking the count of the occuring words instead of just labelling it by occurence. This is called the raw frequency and can be normalized in a couple of ways by the term frequency (tf) model
     
      \begin{equation}
        f(t,d) = c
      \end{equation}

    where \emph{c} is the total count of term \emph{t} occuring in document \emph{d}.
    And a general term frequency function that helps against long document bias by normalizing with the maximum frequency of any occuring word in d.

    \begin{equation}
      tf(t,d) = 0.5 + \frac{0.5 * f(t,d)}{max(f(w,d)\, \forall w \in d)}
    \end{equation}

    The term frequency model can be further advanced by caculating the inverse document frequency (idf).

    \begin{equation}
      idf(t, C) = log(\frac{|C|}{|\forall d \in C : t \in d|})
    \end{equation}

    Where |C| is the size of the corpus and we check for every document in the corpus if the term occurs in the document. The intuition is: How often does a term occur in other documents. If a term appears more often then it is a common word such as ``the'', whereas ``super-symmetry'' might be a rare word. It is therefore some measure of importance. Because idf and tf only measure either importance accross all documents or importance of one document, we need to find words that are not rare and not common either. Generalizing the term-frequency and inverse-document-frequency we obtain the tf-idf:

    \begin{equation}
      tfidf(t, d, D) = tf(t, d) * idf(t, D)
    \end{equation}

    The tf-idf has a high score, if a term occurs often in a single document and less often in other documents. This translates to the notion that a term represents the current document better than other terms and are therefore highly discriminative words.\\

    There are other models such as graph based or tree based approaches. For the porpuse of this thesis these models are left out. @TODO:Ref

    One of the big problems with the vector space model is that feature inflation or feature explosion arises quickly. If documents have a high variance between other documents and the connections between two documents are small the sparsity can go up to 100\%. That means that no document has overlapping words and each document only accounts for the words that were originally in the document. Think of it another way, suppose one has 3 documents in 3 dimensional space with each axis set to zero but one (1,0,0), (0,1,0), (0,0,1) we have an independant basis. This means that all documents are orthogonal to each other, meeting at no point except at the null point (0,0,0). It is therefore not really possible to derive any connections between those 3 points. Thus each document represents its own topic.
    These kind of problems need a well thought out solution which is presented later in this section.

  \subsection{Similarity and Distances}
    \emph{Partitional clustering} algorithms commonly work through some objective distance or similarity function between two objects. After lifting the documents into vector space, we are faced with the problem of distance between two documents. Several measures were proprosed that can be easily interpreted by geometry or linear algebra. Our goal is to have an \emph{m x m} matrix for \emph{document x document} distances. A \emph{document x terms} matrix on the other hand is highly relevant when clustering under the assumption of topical classification. That is compare documents not to each other but to all the occuring words as topics. LSA is an example where words are projected to lower dimensions resembling partial categorization to topics of a document by the most distinct words. Then a clustering can be done by documents that highly correlate to the same topics.

    \paragraph{Cosine similarity}
    Given two documents $d_1$ and $d_2$ the classical cosine similarity is defined by

    \begin{equation}
      cos(d_1, d_2) = \frac{d_1 * d_2}{||d_1|| * ||d_2||}
    \end{equation}

    If the documents are already defined unit vectors cosine similarity is just $cos(d_1, d_2) = d_1 * d_2$. The cosine similarity is a measure of orientation, not the magnitude. The angles between documents is compared and thus if the angle is 0° both documents are equal in size and word occurences. On the other hand we often like to measure not only the direction but the magnitude.

    \paragraph{Euclidean distance ($l^2\:norm$)}
    Again given two documents $d_1$ and $d_2$ it is the geometrical distance between two vectors in $\mathbf{R}^n$. This results in the fact that documents running into different directions, e.g. have different angles might as well be very close. It is a geometric measure and accounts for magintude.

    \begin{equation}
      euclid(d_1, d_2) = \sqrt{\sum_{i=1}^{M}(d_1^i * d_2^i)^2}
    \end{equation}

    \paragraph{Manhattan ($l^1\:norm$)}
    Is commonly known under the city block distance measure. Equally squared brackets
    will get you closest to a document by comparing one dimension at a time and summing up their totals.

    \begin{equation}
      manhattan(d_1, d_2) = \sum_{i=1}^{M}|d_1^i - d_2^i|
    \end{equation}

    There are more distance functions such as the \emph{Jaccard coefficient} that works on intersections of sets and the \emph{Chebyshev distance} which is a maximizing greedy strategy of the manhattan distance. The underlying concepts of similarity should be clear though. Each coefficient works good on a particular set, however cosine and euclidean distances are the commonly used choices.

    \emph{Hierarchical algorithms} use different metrics. The goal here is not to find the closest points but to organize them into hierarchies. A different set of similarity metrics must be used. We will review them later as their functionality strays away from the vector space.

  \subsection{Enhancing the Vector Space Model}

    The vector space model comes with a variety of problems. First we have extremely high dimensions. As we see later this is particularly true for newspapers. Unlike Twitter the content of a newspaper article can be substantial and range over a variety of topics. This can lead to extremely poor clustering results. Several methods have been proposed to tackle this high dimensionality problem. One of the most strickening comes from linear algebra and was initially proposed in \cite{DeerwesterLSI1990} called Latent Semantic Analysis (LSA). It is based on Singular Value Decomposition (SVD). The Principal Component Analysis (PCA) on the other hand uses SVD in a different manner. Apart from these there are also much more elaborate techniques in the domain of topic modelling which will be reviewed later.

    \subsubsection{Latent Semantic Analysis}
      Latent Semantic Analysis is a progress to reduce as much noise as possible from a given document x term matrix to expose the so called latent variables. It is connected to topic modelling but seems to be more relevant in the context of dimensionality reduction. Dimensionality reduction is often proposed for dealing with large sparse data to find the underlying connecting relations. \emph{LSA} is a truncated singular value decomposition \emph{(SVD)} on a term x document matrix. \emph{SVD} is a matrix decomposition where an n x m matrix \emph{A} is decomposed into several parts:

      \begin{equation}
        svd(A) = U_{mxm}\Sigma_{mxn} V'_{nxn}
      \end{equation}

      And for \emph{LSA}

      \begin{equation}
        lsa(A, k) = U_{mxk}\Sigma_{kxk} V'_{kxn}
      \end{equation}

      The difference here really is the resulting dimensions. While in the original \emph{SVD} we keep all components decomposing their respective singular values, \emph{LSA} only keeps \emph{k} components reducing the dimensions from \emph{m} words to \emph{k}.

      The intuition behind \emph{SVD} is to expose as much variance as possible at the same time reducing noise from the data. For a \emph{term x document} matrix this means that a lot of documents provide a lot of words. The higher the variance between each document the more single words there will be. Thus most of the words do not contribute anything to the relationships between documents. SVD then is a natural progress to reduce highly sparse data projecting it to a lower dimensional space. The \emph{SVD} will also sort the words by highest impact and so smaller \emph{singular values} will appear in the lower part of the resulting decomposition.

      Documents that share a particular topic are more similar. This results in connections between documents even if a lot of the words share no common meaning.\\
      Effectively \emph{LSA} tackles problems of synonymy and polysemy. Synonymy means that several words often share the same meaning such as ``big'' and ``large''. While polysemy refers to the fact that one word can have several meanings such as ``bank'' as a fincancial institution and ``bank'' as an object to sit on.
      On the contrary to this \emph{LSA} reduces dimensions so we can compare documents more rigurously. This also means that we destroy variance. If the variance is 100\% before \emph{LSA} and after only 20\% remain, we only account for $frac{1}{5}$ of the original variance. Often this is not desired and we need to find a good \emph{k} to reduce the dimensions to. In laymans terms: we do not want ``happy'' to be close to ``murder'' to be close to ``ordinary''. That is, no meaning but a single one is left.
      In summarization \emph{LSA} is much more frequently used. \cite{SumLSASteinberger2004} proposed a solution to retrieve the highest weighting sentences by \emph{LSA} taking the top \emph{k} words into account as well.

      \begin{figure}[h!]
        \centering
          \includegraphics[width=0.7\textwidth]{svd_lsa.png}
          \caption{"Singular Value Decomposition by \cite{SumLSASteinberger2004}"}
          \label{svd_lsa}
      \end{figure}

      The $\sigma_1 \geq \sigma_2..\sigma_k$ stands for \emph{k} preceeding singular values sorted by magnitude. \emph{LSA} gives us a mapping from the top k topics of the document to the most important sentences of the document. In descending order the document presents less information per sentence. Thusly taking the top k sentences to create summarization can lead to significat results in a summarization processor. \cite{SumLSASteinberger2004}

    \subsubsection{Principal Component Analysis (PCA)}
    The \emph{Principal Component Analysis (PCA)} is a mulativariate technique seperating out correlating variables into principal components. \emph{PCA} in general is a dimensionality reduction technique reducing the number of features to a lesser set of features. Often \emph{SVD} is part of this process. What makes \emph{PCA} different to \emph{LSA} is the underlying methodology.\\
    It uses least square projections to project the data into a plane for each dimension \cite{Strang2009}. Least square projections are useful for linear combinations of vectors that do not intersect. It is then possible to project data onto a line or a plane that can by some error value be part of a combined dimension. Suppose two vectors $["he", "is", "happy"]\:and\:["she", "is", "happy"]$. Both vectors share some meaning and they will result in some form in vector space. \emph{PCA} now takes these vectors and projects ``is'' and ``happy'' onto the same dimension, reducing the overall dimensions by 1. As with the \emph{SVD} we find several components that resemble the data without parts of the noisy variance.

    \begin{figure}[h!]
      \centering
        \includegraphics[width=0.7\textwidth]{PCA.png}
        \caption{"Principal Component Analysis by \cite{PCA2009}"}
        \label{pca}
    \end{figure}

    In this case we seek to find principal components for m documents by n new principal components. Primarily \emph{PCA} can be described as a dimensionality reduction technique that resembles the original structure rather well and can be used for visualization, reducing down to 2 or 3 dimensions. See more in \cite{PCA2009}.


\section{Clustering algorithms}
  
  Clustering algorithms are at the heart of this thesis. Unsupervised methods forming groups of coherent similarity and dissimilarity. Traditional clustering algorithms stray much further than this. They come in shape of \emph{partitional} models, that flatten the structure, in \emph{hierarchical} models that build tree like structures, \emph{spectral} algorithms that are \emph{graph based} or \emph{density} based algorithms. There is also a fifth category namely \emph{generative models} and closely related in terms of the text domain \emph{probabilistic topic models}. These models try to find the underlying latent semantics by generating the model that could have created the documents in the first place. As this is quite a chapter on its own and does not fall under the classic definitions of clustering algorithms they come in a separate part later.\\
  First let us setup a few definitions that all clustering algorithms incorporate.

  \begin{itemize}
    \item \emph{Hard and Soft clustering} - A clustering algorithm is \emph{hard} if documents can only be assigned to one distinct cluster. On the contrary they are \emph{soft} (often called \emph{fuzzy/overlapping}) if a document can have multiple assignments that resemble topical proportions of the document.
    \item \emph{Exhaustive and non-exhaustive} - A clustering is exhaustive if every document has an assignment after the run whereas it is non-exhaustive if documents might not have any assignments at all (assigned to a null cluster).
    \item \emph{Cost functions} - Most clustering algorithms incorporate some sort of cost function that needs to be minimized or maximized after the algorithm completed. The clustering can be rerun several times finding a maximum between different runs by varying the parameters. This should not be confused with the similarity measure between objects during a clustering run (although this can be the objective cost function of a clustering algorithm as well).
    \item \emph{local minima/global optimum} - A local minima is when the objective cost functions between objects do not yield better results after several iterations. This does not mean that the best state is found. Similarity metrics between objects are inherently heuristic and have no view of a global optimum. A global optimum can be approximated by objective cost functions rerunning the algorithm several times, varying the parameters in hope of better results. There is currently no known algorithm that can solve clustering in a deterministic globally optimized way. It is NP-hard.
    \item \emph{Cluster cardinality} - Most of the clustering algorithm incoporate a fixed non optional parameter k that determines how many clusters should be build up. Only a few algorithms such as Birch, Mean-Shift or certain probabilistic topic models can infer the cardinality of a cluster algorithm by tweeking threshold, density or graph based objective cost parameters.
  \end{itemize}

  \subsection{Partitional clustering}
    \emph{Partitional clustering} algorithms build up centroids. They resemble typical cluster centers. After documents are converted into vector space and some sort of similarity measure is applied, cluster centers are incrementally created. We briefly review two partitional clustering algorithms namely \emph{K-Means} and \emph{Expectation Maximization (EM)}. Literature in this area is rigorous, see \cite{ClusteringBooAggarwalk2013, NextFrontierClustering2013, IRBookStanford2008}.\\

    \emph{Partitional algorithms} in particular can be globally approximated given a numerical cost function $f:D \to \mathbf{R}$ such that it maximizes $f(x_0) \geq f(x), \forall x \in D$  or minimizes $f(x_0) \leq f(x), \forall x \in D$. Often they are much easier to implement because constraints are kept low and requirements are kept to a minimum. There are also online versions available which makes it possible to scale algorithms such as K-means with \emph{map reduce} on several processors. This is not true for hierarchical algorithms which build up a hierarchy bottom-up or top-down.\\

    \paragraph{K-Means}
    The \emph{K-Means} clustering algorithm is a centroid based algorithm. Each document is assigned to a cluster center in each iteration. In order to do this \emph{k} centroids are drawn as a initial cluster centers from the documents. Then interdistance similarity measures, typically $l_2\:norm$, decide if a document is closer to a certain centroid. Centroids will be moved to the new location in vector space by averaging over the assigned documents. In the next iteration a document then might be reconsidered for another cluster.\emph{K-Means} typically forms cluster centers of equal sizes and is a hard clustering algorithm \cite{IRBookStanford2008}.\\
    Typically a \emph{K-Means} algorithm will be run with respect to an objective cost function to be minimized. Globally this can be used for chosing a suitable \emph{k} for the underlying data. The objective goal is to minimize the least sum square of within cluster distances \cite{IRBookStanford2008}:

      \begin{equation}
        RSS_k = \sum_{\vec{x}_k \in \textit{w}_k}|\vec{x} - \vec{\mu}(\textit{w}_k)|^{2}
      \end{equation}

    If we now sum over all $RSS_k$ the total amount must be minimized. The algorithm converges either after not improving above a \emph{threshold} or by limiting the iterations by a paramter \emph{max iter} after which the algorithm stops. In this case both criterias are modelled which resembles the implementation by \cite{ScikitLearn} in Sklearn.

    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \caption{$X$ is a document term matrix, $\mu$ is a matrix of centroid vectors, $c$ a mapping between $X$ and $\mu$}\label{kmeans}
      \Function{Kmeans}{$X={x_1,..x_m},k,maxiter$}
        \State $\mu \gets select\:k\:initial\:centroids\:from\:X$
        \State $c \gets init\:assignment\:matrix {}$
        \State $cost \gets \infty$

        \For{$i\gets 1, maxiter$}
          \State $c \gets argmin|\vec{\mu} - \vec{X}|$ \Comment{assign X to k nearest centroids}
          \State $\mu \gets average\:X\:over\:assigned\:\mu\:by\:c$
          \State $cost \gets \sum_{k = 1}^{K}RSS_k$
          \If{$cost\:lead\:to\:convergence$}
            \State Stop
          \EndIf
        \EndFor
        \State \Return $labels, centroids, cost$
      \EndFunction
    \end{algorithmic}
    \end{algorithm}

    \emph{Now we are left with one problem}: How do we chose the initial clusters? Several strategies have been proposed. The most used is random sampling where \emph{k} random documents are chosen as a centroid. The problem is, when all documents are skewed to one direction or close together the \emph{K-means} converges at a local minimum that seems to suboptimally divide the documents into clusters.
    Another strategy has been proposed by \cite{KMeansPlusPlus2007} that maximizes the intercluster distance by spreading the clusters as far from each other as possible. 
    As the initial clusters are often picked randomly \emph{K-Means} is a non deterministic approach that will most likely result in different clusters after each run.

    \paragraph{Expectation Maximization (EM)}
    The \emph{Expecation Maximization (EM)} algorithm is a generalization of the \emph{K-Means} and is a soft clustering algorithm with fuzzy associations between clusters and documents. The underlying methodology is that we would like to find a model that generated the given documents. The way this works is first by assuming that the clusters are represented as distributions over terms. They can be assumed as a normal gaussian distribution. Note: the kind of distribution can vary, gaussian distributions can tend to zero if no assignment was found and so EM could assign no label at all. If that is not desired and a soft and exhaustive clustering is prefered one can add additive smoothing, that other models like the Dirichlet model inheret. What we would like to know is: How does a data point \emph{x} relate to the different \emph{k} guassian distributions? Formally we need to estimate a model $\Theta$ that is maximized using a cost function such as \emph{maximum likelihood estimation (MLE)} given some data. The intution is, by estimating the likelihood of a given incomplete dataset $D$, that is assumed to be normally distributed, we want to maximize the connection between $\Theta$ and $D$.\\

      \begin{equation}
        \Theta = argmax\:\sum_{n=1}^{N}log\:P(d_n,\Theta)
      \end{equation}

    Where $P$ is the fractional probability that a document $d_n$ was generated by $\Theta$. In the text domain this means: for all words in each $D$ can we generate assignments to distributions that maximize $\Theta$.
    The \emph{EM} algorithm tries to maximize this function by assigning a probability that a data point $x$ is likely to be in the cluster $c_1..c_k$ by calculating the joint probabilities of occuring words given a prior of cluster $c_1..c_k$. This is also called the expectation step. In the second step each probability of a document \emph{d} being part of a cluster $c_1..c_k$ is weighted into the average and variance of the defined clusters. The clusters are then recalculated using the beforementioned assignments. This step is repeated until some kind of convergence criteria has been met. The main intuition is if we have a model $\Theta$ we can compute the fractional probabilities of a document $d$ to be in a cluster $k$:

      \begin{equation}
        P(d|\Theta) = \sum_{k=1}^{K}\alpha_k\left ( \prod_{t_m \in d} P(t_m = 1|c_k) \right ) \left ( \prod_{t_m \not\in d} (1 - P(t_m = 1|c_k)) \right )
      \end{equation}

    In the maximization step we then need to compute the probability that a term $t_m$ has a high probability to be in $c_k$ that is $P(t_m = 1|c_k)$ and the prior $\alpha_k$ that is the probability that any document is in $c_1..c_k$ given no evidence. In the expectation step we compute $r_{nk}$ that is the soft assignment of a document $d_n$ to a cluster $k$. $r_{nk}$ is computed with the soft assignments of the prior $\alpha_k$ and $P(t_m = 1|c_k)$.\\
    A final inherent problem remains as was seen with the \emph{K-Means}: How do we choose the initial seed? Expectation Maximization is self referencing so to begin an iteration the seed is most often just $k$ randomly build up distributions that hopefully will converge well. For further information see \cite{IRBookStanford2008}.

  \subsection{Hierarchical / Agglomerative clustering}
    \paragraph{Ward, Complete and Average Linkage}
    \paragraph{Birch}

    \subsection{Others}
      \begin{enumerate}
        \item \emph{Spectral} - x
        \item \emph{Density} - x
        \item \emph{Grid} - x
      \end{enumerate}

\section{Clustering evaluation}

  \subsection{Internal measures}
    Without labels of the ground truth

    \begin{enumerate}
      \item \emph{Silhouette coefficient} - x
      \item \emph{Davies–Bouldin index} - x
      \item \emph{Dunn index} - x
    \end{enumerate}

  \subsection{External measures}
    With labels of the ground truth

    \begin{enumerate}
      \item \emph{F-Measure} - x
      \item \emph{Jaccard index} - x
    \end{enumerate}

\section{Generative Models}    
  
  EM, Cost functions, general clustering scheme

  \subsection{Topic modelling}
    \paragraph{Bayes Theorem}
    \paragraph{Multinominal Distributions}
    \paragraph{Dirichlet Distributions}
      Chinese Restaurant Process

  \subsection{Methods}
    \subsubsection{Latent Dirichlet Allocation (LDA)}
    \subsubsection{Non Negative Matrix Factorization (NMF)}


