@misc{Nothing,
    author = {NoAuthor},
    title = {{Nothing}},
    year = {2015}
}

@misc{NewsBlaster2002,
    author = {McKeown, Kathleen R. and Barzilay, Regina and Evans, David and Hatzivassiloglou, Vasileios and Klavans, Judith L. and Nenkova, Ani and Sable, Carl and Schiffman, Barry and Sigelman, Sergey and Summarization, Multidocument},
    citeulike-article-id = {13842751},
    keywords = {newsblaster, summarization},
    posted-at = {2015-11-20 14:46:37},
    priority = {2},
    title = {{Tracking and Summarizing News on a Daily Basis with Columbia's Newsblaster}},
    year = {2002}
}

@misc{ColumbiaExperimentsSum2002,
    author = {Schiffman, Barry and Nenkova, Ani and McKeown, Kathleen},
    citeulike-article-id = {13842749},
    keywords = {summarization},
    posted-at = {2015-11-20 14:45:23},
    priority = {2},
    title = {{Experiments in Multidocument Summarization}},
    year = {2002}
}

@inproceedings{ColumbiaMultiDoc2001,
    author = {McKeown, Kathleen R. and Hatzivassiloglou, Vasileios and Barzilay, Regina and Schiffman, Barry and Evans, David and Teufel, Simone},
    booktitle = {In Proceedings of the Document Understanding Conference (DUC01},
    citeulike-article-id = {13842747},
    keywords = {summarization},
    posted-at = {2015-11-20 14:43:31},
    priority = {2},
    title = {{Columbia Multi-Document Summarization: Approach and Evaluation}},
    year = {2001}
}

@inproceedings{Simfinder2001,
    author = {Hatzivassiloglou, Vasileios and Klavans, Judith L. and Holcombe, Melissa L. and Barzilay, Regina and yen Kan, Min and McKeown, Kathleen R.},
    booktitle = {In Proceedings of the NAACL Workshop on Automatic Summarization},
    citeulike-article-id = {13842745},
    keywords = {clustering, simfinder},
    pages = {41--49},
    posted-at = {2015-11-20 14:42:13},
    priority = {2},
    title = {{SIMFINDER: A Flexible Clustering Tool for Summarization}},
    year = {2001}
}

@inproceedings{SumLSASteinberger2004,
    author = {Steinberger, Josef and Je\v{z}ek, Karel},
    booktitle = {In Proc. ISIM '04},
    citeulike-article-id = {13842744},
    keywords = {models, topic},
    pages = {93--100},
    posted-at = {2015-11-20 14:40:54},
    priority = {2},
    title = {{Using latent semantic analysis in text summarization and summary evaluation}},
    year = {2004}
}

@inproceedings{SumLSA2001,
    address = {New York, NY, USA},
    author = {Gong, Yihong and Liu, Xin},
    booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    citeulike-article-id = {13842741},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/383952.383955},
    citeulike-linkout-1 = {http://doi.acm.org/10.1145/383952.383955},
    doi = {10.1145/383952.383955},
    keywords = {analysis, generic, measure, models, relevance, semantic, summarization, text, topic},
    location = {New Orleans, Louisiana, USA},
    pages = {19--25},
    posted-at = {2015-11-20 14:38:43},
    priority = {2},
    publisher = {ACM},
    series = {SIGIR '01},
    title = {{Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis}},
    url = {http://doi.acm.org/10.1145/383952.383955},
    year = {2001}
}

@article{PLSA2001,
    abstract = {{This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.}},
    address = {Hingham, MA, USA},
    author = {Hofmann, Thomas},
    citeulike-article-id = {453923},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=599631},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    keywords = {models, topic},
    month = jan,
    number = {1-2},
    pages = {177--196},
    posted-at = {2015-11-20 14:36:04},
    priority = {5},
    publisher = {Kluwer Academic Publishers},
    title = {{Unsupervised Learning by Probabilistic Latent Semantic Analysis}},
    url = {http://portal.acm.org/citation.cfm?id=599631},
    volume = {42},
    year = {2001}
}

@article{LDA2003,
    abstract = {{We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.}},
    address = {Cambridge, MA, USA},
    author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
    citeulike-article-id = {353473},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944937},
    citeulike-linkout-1 = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    doi = {10.1162/jmlr.2003.3.4-5.993},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    keywords = {models, topic},
    month = mar,
    number = {4-5},
    pages = {993--1022},
    posted-at = {2015-11-20 14:35:06},
    priority = {5},
    publisher = {JMLR.org},
    title = {{Latent Dirichlet Allocation}},
    url = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    volume = {3},
    year = {2003}
}

@article{PCA2010,
    author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
    citeulike-article-id = {13842739},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/wics.101},
    doi = {10.1002/wics.101},
    journal = {{WIREs} Comp Stat},
    keywords = {decomposition},
    month = jun,
    number = {4},
    pages = {433--459},
    posted-at = {2015-11-20 14:33:39},
    priority = {2},
    publisher = {Wiley-Blackwell},
    title = {{Principal component analysis}},
    url = {http://dx.doi.org/10.1002/wics.101},
    volume = {2},
    year = {2010}
}

@article{NMF1999,
    abstract = {{Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.}},
    author = {Lee, Daniel D. and Seung, H. Sebastian},
    citeulike-article-id = {423053},
    citeulike-linkout-0 = {http://dx.doi.org/10.1038/44565},
    citeulike-linkout-1 = {http://dx.doi.org/10.1038/401788a0},
    day = {21},
    doi = {10.1038/44565},
    issn = {0028-0836},
    journal = {Nature},
    keywords = {decomposition},
    month = oct,
    number = {6755},
    pages = {788--791},
    posted-at = {2015-11-20 14:26:56},
    priority = {5},
    publisher = {Nature Publishing Group},
    title = {{Learning the parts of objects by non-negative matrix factorization}},
    url = {http://dx.doi.org/10.1038/44565},
    volume = {401},
    year = {1999}
}

@article{TopicModelsBlei2012,
    author = {Blei, David M.},
    citeulike-article-id = {13842732},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/2133806.2133826},
    doi = {10.1145/2133806.2133826},
    journal = {Communications of the ACM},
    keywords = {models, topic},
    month = apr,
    number = {4},
    pages = {77},
    posted-at = {2015-11-20 14:24:31},
    priority = {2},
    publisher = {Association for Computing Machinery (ACM)},
    title = {{Probabilistic topic models}},
    url = {http://dx.doi.org/10.1145/2133806.2133826},
    volume = {55},
    year = {2012}
}

@book{ClusteringBooAggarwalk2013,
    citeulike-article-id = {13842730},
    citeulike-linkout-0 = {http://amazon.com/o/ASIN/1466558210/},
    edition = {0},
    editor = {Aggarwal, Charu C. and Reddy, Chandan K.},
    keywords = {clustering},
    month = aug,
    posted-at = {2015-11-20 14:20:52},
    priority = {2},
    publisher = {Chapman and Hall/CRC},
    title = {{Data Clustering: Algorithms and Applications (Chapman \& Hall/CRC Data Mining and Knowledge Discovery Series)}},
    url = {http://amazon.com/o/ASIN/1466558210/},
    year = {2013}
}

@book{Strang2009,
    author = {Strang, Gilbert},
    citeulike-article-id = {13842708},
    citeulike-linkout-0 = {http://amazon.com/o/ASIN/0980232716/},
    edition = {4},
    keywords = {algebra, linear},
    month = feb,
    posted-at = {2015-11-20 14:04:33},
    priority = {0},
    publisher = {Wellesley Cambridge Press},
    title = {{Introduction to Linear Algebra, Fourth Edition}},
    url = {http://amazon.com/o/ASIN/0980232716/},
    year = {2009}
}

@book{IRBook2008,
    abstract = {{Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.}},
    address = {New York, NY, USA},
    author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
    citeulike-article-id = {4469058},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1394399},
    isbn = {0521865719, 9780521865715},
    keywords = {information, retrieval},
    posted-at = {2015-11-20 13:24:31},
    priority = {3},
    publisher = {Cambridge University Press},
    title = {{Introduction to Information Retrieval}},
    url = {http://portal.acm.org/citation.cfm?id=1394399},
    year = {2008}
}

@inproceedings{SemanticTopicModels2011,
    address = {Edinburgh, Scotland, UK.},
    author = {Guo, Weiwei and Diab, Mona},
    booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
    citeulike-article-id = {11842790},
    citeulike-linkout-0 = {http://www.aclweb.org/anthology/D11-1051},
    keywords = {models, topic},
    month = jul,
    posted-at = {2015-11-09 12:08:16},
    priority = {5},
    publisher = {Association for Computational Linguistics},
    title = {{Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions}},
    url = {http://www.aclweb.org/anthology/D11-1051},
    year = {2011}
}

@incollection{ProbTopicModelsSteyvers2006,
    author = {Steyvers, Mark and Griffiths, Tom},
    booktitle = {Latent Semantic Analysis: A Road to Meaning.},
    citeulike-article-id = {13833786},
    citeulike-linkout-0 = {http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf},
    editor = {Landauer, T. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
    keywords = {models, topic},
    posted-at = {2015-11-09 11:58:38},
    priority = {5},
    publisher = {Laurence Erlbaum},
    title = {{Probabilistic Topic Models}},
    url = {http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf},
    year = {2006}
}

@article{blei2011introduction,
    author = {Blei, David M.},
    citeulike-article-id = {13833783},
    citeulike-linkout-0 = {http://www.cs.princeton.edu/\~{}blei/papers/Blei2011.pdf},
    journal = {Communications of the ACM},
    keywords = {models, topic},
    posted-at = {2015-11-09 11:56:37},
    priority = {5},
    title = {{Introduction to Probabilistic Topic Models}},
    url = {http://www.cs.princeton.edu/\~{}blei/papers/Blei2011.pdf},
    year = {2011}
}

@article{ScikitLearn,
    author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    citeulike-article-id = {13833334},
    journal = {Journal of Machine Learning Research},
    keywords = {api, tools},
    pages = {2825--2830},
    posted-at = {2015-11-08 16:18:20},
    priority = {2},
    title = {Scikit-learn: Machine Learning in {P}ython},
    volume = {12},
    year = {2011}
}

@inproceedings{DeerwesterLSI1990,
    abstract = {{A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\^{a}semantic structure\^{a}) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.}},
    author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
    booktitle = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
    citeulike-article-id = {3998049},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490},
    journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
    keywords = {models, topic},
    number = {6},
    pages = {391--407},
    posted-at = {2015-11-08 16:16:52},
    priority = {5},
    title = {{Indexing by latent semantic analysis}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490},
    volume = {41},
    year = {1990}
}

@inproceedings{gensim2010,
    address = {Valletta, Malta},
    author = {Rehr uv rek and Sojka, Petr},
    booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
    citeulike-article-id = {13833333},
    day = {22},
    keywords = {gensim, lsa, python},
    month = may,
    note = {\\urlhttp://is.muni.cz/publication/884893/en},
    pages = {45--50},
    posted-at = {2015-11-08 16:15:26},
    priority = {5},
    publisher = {ELRA},
    title = {{Software Framework for Topic Modelling with Large Corpora}},
    year = {2010}
}

@article{FeatureSelection,
    abstract = {{CiteSeerX - Document Details (Isaac Councill, Lee Giles):}},
    author = {Alelyani, Salem and Tang, Jiliang and Liu, Huan},
    booktitle = {Data Clustering: Algorithms and Applications},
    citeulike-article-id = {13833332},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.8115},
    keywords = {clustering, feature, selection},
    posted-at = {2015-11-08 16:13:25},
    priority = {5},
    title = {{Feature Selection for Clustering: A Review}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.8115},
    year = {2013}
}

@article{NextFrontierClustering2013,
    abstract = {{CiteSeerX - Document Details (Isaac Councill, Lee Giles):}},
    author = {Anastasiu, David C. and Tagarelli, Andrea and Karypis, George},
    citeulike-article-id = {13833331},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.401.8428},
    keywords = {clustering},
    posted-at = {2015-11-08 16:12:21},
    priority = {5},
    title = {{Document Clustering: The Next Frontier}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.401.8428},
    year = {2013}
}

@book{NLPBookJurafsky2000,
    abstract = {{This book takes an empirical approach to language processing, based on
applying statistical and other machine-learning algorithms to large
corpora.**\_Methodology\_** boxes are included in each chapter. **Each chapter
is built around one or more worked examples\_** to demonstrate the main idea of
the chapter. Covers the fundamental algorithms of various fields, whether
originally proposed for spoken or written language to demonstrate how the same
algorithm can be used for speech recognition and word-sense disambiguation.
Emphasis on web and other practical applications. Emphasis on scientific
evaluation. Useful as a reference for professionals in any of the areas of
speech and language processing.}},
    author = {Jurafsky, Daniel and Martin, James H.},
    citeulike-article-id = {263033},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0130950696},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0130950696},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0130950696},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0130950696},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0130950696/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0130950696},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0130950696},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0130950696},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0130950696\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0130950696},
    day = {05},
    edition = {1},
    howpublished = {Library Binding},
    isbn = {9780130950697},
    keywords = {general, nlp},
    month = feb,
    posted-at = {2015-10-02 09:33:15},
    priority = {5},
    publisher = {Prentice Hall},
    title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (Prentice Hall Series in Artificial Intelligence)}},
    url = {http://www.worldcat.org/isbn/0130950696},
    year = {2000}
}

@incollection{ClusterAlgoSurveyIBM,
    abstract = {{Clustering is a widely studied data mining problem in the text domains. The problem finds numerous applications in customer segmentation, classification, collaborative filtering, visualization, document organization, and indexing. In this chapter, we will provide a detailed survey of the problem of text clustering. We will study the key challenges of the clustering problem, as it applies to the text domain. We will discuss the key methods used for text clustering, and their relative advantages. We will also discuss a number of recent advances in the area in the context of social network and linked data.}},
    address = {Boston, MA},
    author = {Aggarwal, CharuC and Zhai, ChengXiang},
    booktitle = {Mining Text Data},
    chapter = {4},
    citeulike-article-id = {10678406},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-1-4614-3223-4\_4},
    citeulike-linkout-1 = {http://www.springerlink.com/content/v576v8x63031kr88},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/978-1-4614-3223-4\_4},
    doi = {10.1007/978-1-4614-3223-4\_4},
    editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
    isbn = {978-1-4614-3222-7},
    keywords = {clustering},
    pages = {77--128},
    posted-at = {2015-10-02 09:31:12},
    priority = {5},
    publisher = {Springer US},
    title = {{A Survey of Text Clustering Algorithms}},
    url = {http://dx.doi.org/10.1007/978-1-4614-3223-4\_4},
    year = {2012}
}

@article{NounPhraseSemanticClustering@2009@Zheng,
    abstract = {{Text document clustering plays an important role in providing better document retrieval, document browsing, and text mining. Traditionally, clustering techniques do not consider the semantic relationships between words, such as synonymy and hypernymy. To exploit semantic relationships, ontologies such as WordNet have been used to improve clustering results. However, WordNet-based clustering methods mostly rely on single-term analysis of text; they do not perform any phrase-based analysis. In addition, these methods utilize synonymy to identify concepts and only explore hypernymy to calculate concept frequencies, without considering other semantic relationships such as hyponymy. To address these issues, we combine detection of noun phrases with the use of WordNet as background knowledge to explore better ways of representing documents semantically for clustering. First, based on noun phrases as well as single-term analysis, we exploit different document representation methods to analyze the effectiveness of hypernymy, hyponymy, holonymy, and meronymy. Second, we choose the most effective method and compare it with the WordNet-based clustering method proposed by others. The experimental results show the effectiveness of semantic relationships for clustering are (from highest to lowest): hypernymy, hyponymy, meronymy, and holonymy. Moreover, we found that noun phrase analysis improves the WordNet-based clustering method.}},
    address = {New York, NY, USA},
    author = {Zheng, Hai T. and Kang, Bo Y. and Kim, Hong G.},
    citeulike-article-id = {7163293},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1531037},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.ins.2009.02.019},
    doi = {10.1016/j.ins.2009.02.019},
    issn = {0020-0255},
    journal = {Inf. Sci.},
    keywords = {clustering, notavailable, notread},
    month = jun,
    number = {13},
    pages = {2249--2262},
    posted-at = {2015-04-24 17:30:05},
    priority = {1},
    publisher = {Elsevier Science Inc.},
    title = {{Exploiting Noun Phrases and Semantic Relationships for Text Document Clustering}},
    url = {http://dx.doi.org/10.1016/j.ins.2009.02.019},
    volume = {179},
    year = {2009}
}

@inproceedings{ClusterRefinementModelSelect@2002@Liu,
    abstract = {{In this paper, we propose a document clustering method that strives to achieve: (1) a high accuracy of document clustering, and (2) the capability of estimating the number of clusters in the document corpus (i.e. the model selection capability). To accurately cluster the given document corpus, we employ a richer feature set to represent each document, and use the Gaussian Mixture Model (GMM) together with the Expectation-Maximization (EM) algorithm to conduct an initial document clustering. From this initial result, we identify a set of discriminative featuresfor each cluster, and refine the initially obtained document clusters by voting on the cluster label of each document using this discriminative feature set. This self-refinement process of discriminative feature identification and cluster label voting is iteratively applied until the convergence of document clusters. On the other hand, the model selection capability is achieved by introducing randomness in the cluster initialization stage, and then discovering a value C for the number of clusters N by which running the document clustering process for a fixed number of times yields sufficiently similar results. Performance evaluations exhibit clear superiority of the proposed method with its improved document clustering and model selection accuracies. The evaluations also demonstrate how each feature as well as the cluster refinement process contribute to the document clustering accuracy.}},
    address = {New York, NY, USA},
    author = {Liu, Xin and Gong, Yihong and Xu, Wei and Zhu, Shenghuo},
    booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    citeulike-article-id = {104234},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=564411},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/564376.564411},
    doi = {10.1145/564376.564411},
    isbn = {1-58113-561-0},
    keywords = {clustering, notread},
    location = {Tampere, Finland},
    pages = {191--198},
    posted-at = {2015-04-24 17:22:14},
    priority = {5},
    publisher = {ACM},
    series = {SIGIR '02},
    title = {{Document Clustering with Cluster Refinement and Model Selection Capabilities}},
    url = {http://dx.doi.org/10.1145/564376.564411},
    year = {2002}
}

@article{WordNetAndFuzzyAssociation@2010@Chen,
    abstract = {{With the rapid growth of text documents, document clustering has become one of the main techniques for organizing large amount of documents into a small number of meaningful clusters. However, there still exist several challenges for document clustering, such as high dimensionality, scalability, accuracy, meaningful cluster labels, overlapping clusters, and extracting semantics from texts. In order to improve the quality of document clustering results, we propose an effective Fuzzy-based Multi-label Document Clustering (FMDC) approach that integrates fuzzy association rule mining with an existing ontology WordNet to alleviate these problems. In our approach, the key terms will be extracted from the document set, and the initial representation of all documents is further enriched by using hypernyms of WordNet in order to exploit the semantic relations between terms. Then, a fuzzy association rule mining algorithm for texts is employed to discover a set of highly-related fuzzy frequent itemsets, which contain key terms to be regarded as the labels of the candidate clusters. Finally, each document is dispatched into more than one target cluster by referring to these candidate clusters, and then the highly similar target clusters are merged. We conducted experiments to evaluate the performance based on Classic, Re0, R8, and WebKB datasets. The experimental results proved that our approach outperforms the influential document clustering methods with higher accuracy. Therefore, our approach not only provides more general and meaningful labels for documents, but also effectively generates overlapping clusters.}},
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Chen, Chun L. and Tseng, Frank S. C. and Liang, Tyne},
    citeulike-article-id = {7839529},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1869133.1869215},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.datak.2010.08.003},
    day = {31},
    doi = {10.1016/j.datak.2010.08.003},
    issn = {0169-023X},
    journal = {Data Knowl. Eng.},
    keywords = {clustering, notread, wordnet},
    month = nov,
    number = {11},
    pages = {1208--1226},
    posted-at = {2015-04-24 17:10:53},
    priority = {5},
    publisher = {Elsevier Science Publishers B. V.},
    title = {{Editorial: An Integration of WordNet and Fuzzy Association Rule Mining for Multi-label Document Clustering}},
    url = {http://dx.doi.org/10.1016/j.datak.2010.08.003},
    volume = {69},
    year = {2010}
}

@article{ColumbiaMultiSumWang2011,
    abstract = {{Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix and then conduct the clustering process. Although many of these clustering methods can group the documents effectively, it is still hard for people to capture the meaning of the documents since there is no satisfactory interpretation for each document cluster. A straightforward solution is to first cluster the documents and then summarize each document cluster using summarization methods. However, most of the current summarization methods are solely based on the sentence-term matrix and ignore the context dependence of the sentences. As a result, the generated summaries lack guidance from the document clusters. In this article, we propose a new language model to simultaneously cluster and summarize documents by making use of both the document-term and sentence-term matrices. By utilizing the mutual influence of document clustering and summarization, our method makes; (1) a better document clustering method with more meaningful interpretation; and (2) an effective document summarization method with guidance from document clustering. Experimental results on various document datasets show the effectiveness of our proposed method and the high interpretability of the generated summaries.}},
    address = {New York, NY, USA},
    author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao and Chi, Yun and Gong, Yihong},
    citeulike-article-id = {13591131},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1993077.1993078},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1993077.1993078},
    doi = {10.1145/1993077.1993078},
    issn = {1556-4681},
    journal = {ACM Trans. Knowl. Discov. Data},
    keywords = {clustering},
    month = aug,
    number = {3},
    posted-at = {2015-04-24 16:28:11},
    priority = {5},
    publisher = {ACM},
    title = {{Integrating Document Clustering and Multidocument Summarization}},
    url = {http://dx.doi.org/10.1145/1993077.1993078},
    volume = {5},
    year = {2011}
}

@misc{SumEvaluation2001,
    abstract = {{This paper provides an overview of different methods
for evaluating automatic summarization systems.
The challenges in evaluating summaries are characterized.
Both intrinsic and extrinsic approaches are
discussed. Methods for assessing informativeness and
coherence are described. The advantages and disadvantages

of specific methods are assessed, along with
criteria for choosing among them. The paper concludes
with some suggestions for future directions.}},
    author = {Mani, I.},
    citeulike-article-id = {390651},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.2078},
    keywords = {summarization},
    posted-at = {2015-11-24 17:11:52},
    priority = {3},
    title = {{Summarization evaluation: An overview}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.2078},
    year = {2001}
}

@inproceedings{Carrot2Search2003,
    author = {Stefanowski, Jerzy and Weiss, Dawid},
    booktitle = {Web Intelligence, First International Atlantic Web Intelligence Conference, {AWIC} 2003, Madrid, Spain, May 5-6, 2003, Proceedings},
    citeulike-article-id = {13845644},
    keywords = {clustering, engine},
    pages = {240--249},
    posted-at = {2015-11-25 13:42:10},
    priority = {2},
    title = {{Carrot and Language Properties in Web Search Results Clustering}},
    year = {2003}
}

@inproceedings{KMeansPlusPlus2007,
    abstract = {{The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.}},
    address = {Philadelphia, PA, USA},
    author = {Arthur, David and Vassilvitskii, Sergei},
    booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
    citeulike-article-id = {3513166},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1283494},
    isbn = {978-0-898716-24-5},
    keywords = {clustering, k-means, seeding},
    location = {New Orleans, Louisiana},
    pages = {1027--1035},
    posted-at = {2009-04-28 03:34:59},
    priority = {0},
    publisher = {Society for Industrial and Applied Mathematics},
    series = {SODA '07},
    title = {{K-means++: The Advantages of Careful Seeding}},
    url = {http://portal.acm.org/citation.cfm?id=1283494},
    year = {2007}
}

@misc{PCA2009,
    author = {Richardson, Mark},
    keywords = {dimensionality reduction},
    posted-at = {2015-11-20 14:45:23},
    title = {{Principal component analysis}},
    year = {2009},
    publisher={University of Oxford},
    location={University of Oxford},
}

@book{IRBookStanford2008,
  Author = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Schütze},
  Title = {Introduction to Information Retrieval},
  Publisher = {Cambridge University Press},
  Year = {2008},
  ISBN = {0521865719},
}

@article{Silhouettes1987,
    abstract = {{An abstract is not available.}},
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Rousseeuw, Peter},
    citeulike-article-id = {1283783},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=38768.38772},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/0377-0427(87)90125-7},
    doi = {10.1016/0377-0427(87)90125-7},
    issn = {0377-0427},
    journal = {J. Comput. Appl. Math.},
    keywords = {evaluation},
    month = nov,
    number = {1},
    pages = {53--65},
    posted-at = {2015-12-05 12:52:48},
    priority = {5},
    publisher = {Elsevier Science Publishers B. V.},
    title = {{Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis}},
    url = {http://dx.doi.org/10.1016/0377-0427(87)90125-7},
    volume = {20},
    year = {1987}
}

@article{DavisBouldin1979,
    abstract = {{A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.}},
    author = {Davies, David L. and Bouldin, Donald W.},
    booktitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
    citeulike-article-id = {4173429},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tpami.1979.4766909},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4766909},
    day = {27},
    doi = {10.1109/tpami.1979.4766909},
    institution = {Department of Electrical Engineering, University of Tennessee, Knoxville, TN 37916; 17 C Downey Drive, Manchester, CT 06040.},
    issn = {0162-8828},
    journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
    month = apr,
    number = {2},
    pages = {224--227},
    posted-at = {2015-12-05 13:15:37},
    priority = {2},
    publisher = {IEEE},
    title = {{A Cluster Separation Measure}},
    url = {http://dx.doi.org/10.1109/tpami.1979.4766909},
    volume = {PAMI-1},
    year = {1979}
}

@article{DunnIndex1973,
    abstract = {{Abstract Two fuzzy versions of the k-means optimal, least squared error partitioning problem are formulated for finite subsets X of a general inner product space. In both cases, the extremizing solutions are shown to be fixed points of a certain operator T on the class of fuzzy, k-partitions of X, and simple iteration of T provides an algorithm which has the descent property relative to the least squared error criterion function. In the first case, the range of T consists largely of ordinary (i.e. non-fuzzy) partitions of X and the associated iteration scheme is essentially the well known ISODATA process of Ball and Hall. However, in the second case, the range of T consists mainly of fuzzy partitions and the associated algorithm is new; when X consists of k compact well separated (CWS) clusters, Xi , this algorithm generates a limiting partition with membership functions which closely approximate the characteristic functions of the clusters Xi . However, when X is not the union of k CWS clusters, the limiting partition is truly fuzzy in the sense that the values of its component membership functions differ substantially from 0 or 1 over certain regions of X. Thus, unlike ISODATA, the ?fuzzy? algorithm signals the presence or absence of CWS clusters in X. Furthermore, the fuzzy algorithm seems significantly less prone to the ?cluster-splitting? tendency of ISODATA and may also be less easily diverted to uninteresting locally optimal partitions. Finally, for data sets X consisting of dense CWS clusters embedded in a diffuse background of strays, the structure of X is accurately reflected in the limiting partition generated by the fuzzy algorithm. Mathematical arguments and numerical results are offered in support of the foregoing assertions.}},
    author = {Dunn, J. C.},
    citeulike-article-id = {6774992},
    citeulike-linkout-0 = {http://dx.doi.org/10.1080/01969727308546046},
    citeulike-linkout-1 = {http://www.tandfonline.com/doi/abs/10.1080/01969727308546046},
    day = {1},
    doi = {10.1080/01969727308546046},
    journal = {Journal of Cybernetics},
    keywords = {evaluation},
    month = jan,
    number = {3},
    pages = {32--57},
    posted-at = {2015-12-05 13:23:47},
    priority = {2},
    publisher = {Taylor \& Francis},
    title = {{A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters}},
    url = {http://dx.doi.org/10.1080/01969727308546046},
    volume = {3},
    year = {1973}
}

@inproceedings{VMeasure2007,
    author = {Rosenberg, Andrew and Hirschberg, Julia},
    booktitle = {{Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)}},
    citeulike-article-id = {1938578},
    citeulike-linkout-0 = {http://www.aclweb.org/anthology-new/D/D07/D07-1043.pdf},
    keywords = {evaluation},
    pages = {410--420},
    posted-at = {2015-12-06 12:24:19},
    priority = {5},
    title = {{V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure}},
    url = {http://www.aclweb.org/anthology-new/D/D07/D07-1043.pdf},
    year = {2007}
}

@article{RandIndex1971,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2284239},
 abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
 author = {William M. Rand},
 journal = {Journal of the American Statistical Association},
 number = {336},
 pages = {846-850},
 publisher = {Taylor & Francis, Ltd.},
 title = {Objective Criteria for the Evaluation of Clustering Methods},
 volume = {66},
 year = {1971}
}

@inproceedings{BIRCH1996,
    abstract = {{Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.}},
    address = {New York, NY, USA},
    author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
    booktitle = {Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data},
    citeulike-article-id = {1015672},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=233269.233324},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/233269.233324},
    doi = {10.1145/233269.233324},
    isbn = {0-89791-794-4},
    issn = {0163-5808},
    keywords = {clustering},
    location = {Montreal, Quebec, Canada},
    pages = {103--114},
    posted-at = {2015-12-07 11:49:23},
    priority = {2},
    publisher = {ACM},
    series = {SIGMOD '96},
    title = {{BIRCH: An Efficient Data Clustering Method for Very Large Databases}},
    url = {http://dx.doi.org/10.1145/233269.233324},
    year = {1996}
}

@book{NltkPython,
    abstract = {{This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify "named entities" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.}},
    author = {Bird, Steven and Klein, Ewan and Loper, Edward},
    citeulike-article-id = {13856792},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1717171},
    edition = {1st},
    isbn = {0596516495, 9780596516499},
    keywords = {python},
    posted-at = {2015-12-07 11:01:39},
    priority = {2},
    publisher = {O'Reilly Media, Inc.},
    title = {{Natural Language Processing with Python}},
    url = {http://portal.acm.org/citation.cfm?id=1717171},
    year = {2009}
}

@book{BishopML,
    abstract = {{An abstract is not available.}},
    address = {Secaucus, NJ, USA},
    author = {Bishop, Christopher M.},
    citeulike-article-id = {1776044},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1162264},
    isbn = {0387310738},
    keywords = {ml},
    posted-at = {2015-12-07 14:54:59},
    priority = {2},
    publisher = {Springer-Verlag New York, Inc.},
    title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
    url = {http://portal.acm.org/citation.cfm?id=1162264},
    year = {2006}
}

@inproceedings{BBCData2006,
  Author = {Derek Greene and P\'{a}draig Cunningham},
  Booktitle = {Proc. 23rd International Conference on Machine learning (ICML'06)},
  Pages = {377--384},
  Publisher = {ACM Press},
  Title = {Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering},
  Year = {2006}
}

@INPROCEEDINGS{NonParametricBayes2007,
    author = {Wei Li and David Blei and Andrew Mccallum},
    title = {Nonparametric bayes pachinko allocation},
    booktitle = {In UAI},
    year = {2007}
}

@article{HDP2006,
  doi = {10.1198/016214506000000302},
  url = {http://dx.doi.org/10.1198/016214506000000302},
  year  = {2006},
  month = {dec},
  publisher = {Informa {UK} Limited},
  volume = {101},
  number = {476},
  pages = {1566--1581},
  author = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
  title = {Hierarchical Dirichlet Processes},
  journal = {Journal of the American Statistical Association}
}

@article{Wordnet1995,
 author = {Miller, George A.},
 title = {WordNet: A Lexical Database for English},
 journal = {Commun. ACM},
 issue_date = {Nov. 1995},
 volume = {38},
 number = {11},
 month = nov,
 year = {1995},
 issn = {0001-0782},
 pages = {39--41},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/219717.219748},
 doi = {10.1145/219717.219748},
 acmid = {219748},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@book{Wordnet1998,
  added-at = {2013-07-22T14:36:51.000+0200},
  author = {Fellbaum, C.},
  biburl = {http://www.bibsonomy.org/bibtex/21896200c29cc99b8980e4e3be8877153/scubbx},
  interhash = {42daa1681607dd1d3f3234c605d84ec3},
  intrahash = {1896200c29cc99b8980e4e3be8877153},
  isbn = {9780262061971},
  keywords = {database electronic lexical wordnet},
  lccn = {97048710},
  publisher = {Mit Press},
  series = {Language, Speech and Communication},
  timestamp = {2013-07-22T14:36:51.000+0200},
  title = {WordNet: An Electronic Lexical Database},
  url = {http://books.google.at/books?id=Rehu8OOzMIMC},
  year = 1998
}

