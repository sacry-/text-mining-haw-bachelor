@misc{NewsBlaster2002,
    author = {McKeown, Kathleen R. and Barzilay, Regina and Evans, David and Hatzivassiloglou, Vasileios and Klavans, Judith L. and Nenkova, Ani and Sable, Carl and Schiffman, Barry and Sigelman, Sergey and Summarization, Multidocument},
    citeulike-article-id = {13842751},
    keywords = {newsblaster, summarization},
    posted-at = {2015-11-20 14:46:37},
    priority = {2},
    title = {{Tracking and Summarizing News on a Daily Basis with Columbia's Newsblaster}},
    year = {2002}
}

@misc{ColumbiaExperimentsSum2002,
    author = {Schiffman, Barry and Nenkova, Ani and McKeown, Kathleen and Summarization, Multidocument},
    citeulike-article-id = {13842749},
    keywords = {summarization},
    posted-at = {2015-11-20 14:45:23},
    priority = {2},
    title = {{Experiments in Multidocument Summarization}},
    year = {2002}
}

@inproceedings{ColumbiaMultiDoc2001,
    author = {McKeown, Kathleen R. and Hatzivassiloglou, Vasileios and Barzilay, Regina and Schiffman, Barry and Evans, David and Teufel, Simone},
    booktitle = {In Proceedings of the Document Understanding Conference (DUC01},
    citeulike-article-id = {13842747},
    keywords = {summarization},
    posted-at = {2015-11-20 14:43:31},
    priority = {2},
    title = {{Columbia Multi-Document Summarization: Approach and Evaluation}},
    year = {2001}
}

@inproceedings{Simfinder2001,
    author = {Hatzivassiloglou, Vasileios and Klavans, Judith L. and Holcombe, Melissa L. and Barzilay, Regina and yen Kan, Min and McKeown, Kathleen R.},
    booktitle = {In Proceedings of the NAACL Workshop on Automatic Summarization},
    citeulike-article-id = {13842745},
    keywords = {clustering, simfinder},
    pages = {41--49},
    posted-at = {2015-11-20 14:42:13},
    priority = {2},
    title = {{SIMFINDER: A Flexible Clustering Tool for Summarization}},
    year = {2001}
}

@inproceedings{SumLSASteinberger2004,
    author = {Steinberger, Josef and Je\v{z}ek, Karel},
    booktitle = {In Proc. ISIM '04},
    citeulike-article-id = {13842744},
    keywords = {models, topic},
    pages = {93--100},
    posted-at = {2015-11-20 14:40:54},
    priority = {2},
    title = {{Using latent semantic analysis in text summarization and summary evaluation}},
    year = {2004}
}

@inproceedings{SumLSA2001,
    address = {New York, NY, USA},
    author = {Gong, Yihong and Liu, Xin},
    booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    citeulike-article-id = {13842741},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/383952.383955},
    citeulike-linkout-1 = {http://doi.acm.org/10.1145/383952.383955},
    doi = {10.1145/383952.383955},
    keywords = {analysis, generic, measure, models, relevance, semantic, summarization, text, topic},
    location = {New Orleans, Louisiana, USA},
    pages = {19--25},
    posted-at = {2015-11-20 14:38:43},
    priority = {2},
    publisher = {ACM},
    series = {SIGIR '01},
    title = {{Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis}},
    url = {http://doi.acm.org/10.1145/383952.383955},
    year = {2001}
}

@article{PLSA2001,
    abstract = {{This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.}},
    address = {Hingham, MA, USA},
    author = {Hofmann, Thomas},
    citeulike-article-id = {453923},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=599631},
    issn = {0885-6125},
    journal = {Mach. Learn.},
    keywords = {models, topic},
    month = jan,
    number = {1-2},
    pages = {177--196},
    posted-at = {2015-11-20 14:36:04},
    priority = {5},
    publisher = {Kluwer Academic Publishers},
    title = {{Unsupervised Learning by Probabilistic Latent Semantic Analysis}},
    url = {http://portal.acm.org/citation.cfm?id=599631},
    volume = {42},
    year = {2001}
}

@article{LDA2003,
    abstract = {{We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.}},
    address = {Cambridge, MA, USA},
    author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
    citeulike-article-id = {353473},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944937},
    citeulike-linkout-1 = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    doi = {10.1162/jmlr.2003.3.4-5.993},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    keywords = {models, topic},
    month = mar,
    number = {4-5},
    pages = {993--1022},
    posted-at = {2015-11-20 14:35:06},
    priority = {5},
    publisher = {JMLR.org},
    title = {{Latent Dirichlet Allocation}},
    url = {http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993},
    volume = {3},
    year = {2003}
}

@article{PCA2010,
    author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
    citeulike-article-id = {13842739},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/wics.101},
    doi = {10.1002/wics.101},
    journal = {{WIREs} Comp Stat},
    keywords = {decomposition},
    month = jun,
    number = {4},
    pages = {433--459},
    posted-at = {2015-11-20 14:33:39},
    priority = {2},
    publisher = {Wiley-Blackwell},
    title = {{Principal component analysis}},
    url = {http://dx.doi.org/10.1002/wics.101},
    volume = {2},
    year = {2010}
}

@article{NMF1999,
    abstract = {{Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.}},
    author = {Lee, Daniel D. and Seung, H. Sebastian},
    citeulike-article-id = {423053},
    citeulike-linkout-0 = {http://dx.doi.org/10.1038/44565},
    citeulike-linkout-1 = {http://dx.doi.org/10.1038/401788a0},
    day = {21},
    doi = {10.1038/44565},
    issn = {0028-0836},
    journal = {Nature},
    keywords = {decomposition},
    month = oct,
    number = {6755},
    pages = {788--791},
    posted-at = {2015-11-20 14:26:56},
    priority = {5},
    publisher = {Nature Publishing Group},
    title = {{Learning the parts of objects by non-negative matrix factorization}},
    url = {http://dx.doi.org/10.1038/44565},
    volume = {401},
    year = {1999}
}

@article{TopicModelsBlei2012,
    author = {Blei, David M.},
    citeulike-article-id = {13842732},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/2133806.2133826},
    doi = {10.1145/2133806.2133826},
    journal = {Communications of the ACM},
    keywords = {models, topic},
    month = apr,
    number = {4},
    pages = {77},
    posted-at = {2015-11-20 14:24:31},
    priority = {2},
    publisher = {Association for Computing Machinery (ACM)},
    title = {{Probabilistic topic models}},
    url = {http://dx.doi.org/10.1145/2133806.2133826},
    volume = {55},
    year = {2012}
}

@book{ClusteringBooAggarwalk2013,
    citeulike-article-id = {13842730},
    citeulike-linkout-0 = {http://amazon.com/o/ASIN/1466558210/},
    edition = {0},
    editor = {Aggarwal, Charu C. and Reddy, Chandan K.},
    keywords = {clustering},
    month = aug,
    posted-at = {2015-11-20 14:20:52},
    priority = {2},
    publisher = {Chapman and Hall/CRC},
    title = {{Data Clustering: Algorithms and Applications (Chapman \& Hall/CRC Data Mining and Knowledge Discovery Series)}},
    url = {http://amazon.com/o/ASIN/1466558210/},
    year = {2013}
}

@book{Strang2009,
    author = {Strang, Gilbert},
    citeulike-article-id = {13842708},
    citeulike-linkout-0 = {http://amazon.com/o/ASIN/0980232716/},
    edition = {4},
    keywords = {algebra, linear},
    month = feb,
    posted-at = {2015-11-20 14:04:33},
    priority = {0},
    publisher = {Wellesley Cambridge Press},
    title = {{Introduction to Linear Algebra, Fourth Edition}},
    url = {http://amazon.com/o/ASIN/0980232716/},
    year = {2009}
}

@book{IRBook2008,
    abstract = {{Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.}},
    address = {New York, NY, USA},
    author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
    citeulike-article-id = {4469058},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1394399},
    isbn = {0521865719, 9780521865715},
    keywords = {information, retrieval},
    posted-at = {2015-11-20 13:24:31},
    priority = {3},
    publisher = {Cambridge University Press},
    title = {{Introduction to Information Retrieval}},
    url = {http://portal.acm.org/citation.cfm?id=1394399},
    year = {2008}
}

@inproceedings{SemanticTopicModels2011,
    address = {Edinburgh, Scotland, UK.},
    author = {Guo, Weiwei and Diab, Mona},
    booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
    citeulike-article-id = {11842790},
    citeulike-linkout-0 = {http://www.aclweb.org/anthology/D11-1051},
    keywords = {models, topic},
    month = jul,
    posted-at = {2015-11-09 12:08:16},
    priority = {5},
    publisher = {Association for Computational Linguistics},
    title = {{Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions}},
    url = {http://www.aclweb.org/anthology/D11-1051},
    year = {2011}
}

@incollection{ProbTopicModelsSteyvers2006,
    author = {Steyvers, Mark and Griffiths, Tom},
    booktitle = {Latent Semantic Analysis: A Road to Meaning.},
    citeulike-article-id = {13833786},
    citeulike-linkout-0 = {http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf},
    editor = {Landauer, T. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
    keywords = {models, topic},
    posted-at = {2015-11-09 11:58:38},
    priority = {5},
    publisher = {Laurence Erlbaum},
    title = {{Probabilistic Topic Models}},
    url = {http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf},
    year = {2006}
}

@article{blei2011introduction,
    author = {Blei, David M.},
    citeulike-article-id = {13833783},
    citeulike-linkout-0 = {http://www.cs.princeton.edu/\~{}blei/papers/Blei2011.pdf},
    journal = {Communications of the ACM},
    keywords = {models, topic},
    posted-at = {2015-11-09 11:56:37},
    priority = {5},
    title = {{Introduction to Probabilistic Topic Models}},
    url = {http://www.cs.princeton.edu/\~{}blei/papers/Blei2011.pdf},
    year = {2011}
}

@article{scikit-learn,
    author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    citeulike-article-id = {13833334},
    journal = {Journal of Machine Learning Research},
    keywords = {api, tools},
    pages = {2825--2830},
    posted-at = {2015-11-08 16:18:20},
    priority = {2},
    title = {Scikit-learn: Machine Learning in {P}ython},
    volume = {12},
    year = {2011}
}

@inproceedings{DeerwesterLSI1990,
    abstract = {{A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\^{a}semantic structure\^{a}) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.}},
    author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
    booktitle = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
    citeulike-article-id = {3998049},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490},
    journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
    keywords = {models, topic},
    number = {6},
    pages = {391--407},
    posted-at = {2015-11-08 16:16:52},
    priority = {5},
    title = {{Indexing by latent semantic analysis}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490},
    volume = {41},
    year = {1990}
}

@inproceedings{gensim2010,
    address = {Valletta, Malta},
    author = {Rehr uv rek and Sojka, Petr},
    booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
    citeulike-article-id = {13833333},
    day = {22},
    keywords = {gensim, lsa, python},
    month = may,
    note = {\\urlhttp://is.muni.cz/publication/884893/en},
    pages = {45--50},
    posted-at = {2015-11-08 16:15:26},
    priority = {5},
    publisher = {ELRA},
    title = {{Software Framework for Topic Modelling with Large Corpora}},
    year = {2010}
}

@article{FeatureSelection,
    abstract = {{CiteSeerX - Document Details (Isaac Councill, Lee Giles):}},
    author = {Alelyani, Salem and Tang, Jiliang and Liu, Huan},
    booktitle = {Data Clustering: Algorithms and Applications},
    citeulike-article-id = {13833332},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.8115},
    keywords = {clustering, feature, selection},
    posted-at = {2015-11-08 16:13:25},
    priority = {5},
    title = {{Feature Selection for Clustering: A Review}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.8115},
    year = {2013}
}

@article{NextFrontierClustering2013,
    abstract = {{CiteSeerX - Document Details (Isaac Councill, Lee Giles):}},
    author = {Anastasiu, David C. and Tagarelli, Andrea and Karypis, George},
    citeulike-article-id = {13833331},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.401.8428},
    keywords = {clustering},
    posted-at = {2015-11-08 16:12:21},
    priority = {5},
    title = {{Document Clustering: The Next Frontier}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.401.8428},
    year = {2013}
}

@book{NLPBookJurafsky2000,
    abstract = {{This book takes an empirical approach to language processing, based on
applying statistical and other machine-learning algorithms to large
corpora.**\_Methodology\_** boxes are included in each chapter. **Each chapter
is built around one or more worked examples\_** to demonstrate the main idea of
the chapter. Covers the fundamental algorithms of various fields, whether
originally proposed for spoken or written language to demonstrate how the same
algorithm can be used for speech recognition and word-sense disambiguation.
Emphasis on web and other practical applications. Emphasis on scientific
evaluation. Useful as a reference for professionals in any of the areas of
speech and language processing.}},
    author = {Jurafsky, Daniel and Martin, James H.},
    citeulike-article-id = {263033},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0130950696},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0130950696},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0130950696},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0130950696},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0130950696/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0130950696},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0130950696},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0130950696},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0130950696\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0130950696},
    day = {05},
    edition = {1},
    howpublished = {Library Binding},
    isbn = {9780130950697},
    keywords = {general, nlp},
    month = feb,
    posted-at = {2015-10-02 09:33:15},
    priority = {5},
    publisher = {Prentice Hall},
    title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (Prentice Hall Series in Artificial Intelligence)}},
    url = {http://www.worldcat.org/isbn/0130950696},
    year = {2000}
}

@incollection{ClusterAlgoSurveyIBM,
    abstract = {{Clustering is a widely studied data mining problem in the text domains. The problem finds numerous applications in customer segmentation, classification, collaborative filtering, visualization, document organization, and indexing. In this chapter, we will provide a detailed survey of the problem of text clustering. We will study the key challenges of the clustering problem, as it applies to the text domain. We will discuss the key methods used for text clustering, and their relative advantages. We will also discuss a number of recent advances in the area in the context of social network and linked data.}},
    address = {Boston, MA},
    author = {Aggarwal, CharuC and Zhai, ChengXiang},
    booktitle = {Mining Text Data},
    chapter = {4},
    citeulike-article-id = {10678406},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-1-4614-3223-4\_4},
    citeulike-linkout-1 = {http://www.springerlink.com/content/v576v8x63031kr88},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/978-1-4614-3223-4\_4},
    doi = {10.1007/978-1-4614-3223-4\_4},
    editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
    isbn = {978-1-4614-3222-7},
    keywords = {clustering},
    pages = {77--128},
    posted-at = {2015-10-02 09:31:12},
    priority = {5},
    publisher = {Springer US},
    title = {{A Survey of Text Clustering Algorithms}},
    url = {http://dx.doi.org/10.1007/978-1-4614-3223-4\_4},
    year = {2012}
}

@article{NounPhraseSemanticClustering@2009@Zheng,
    abstract = {{Text document clustering plays an important role in providing better document retrieval, document browsing, and text mining. Traditionally, clustering techniques do not consider the semantic relationships between words, such as synonymy and hypernymy. To exploit semantic relationships, ontologies such as WordNet have been used to improve clustering results. However, WordNet-based clustering methods mostly rely on single-term analysis of text; they do not perform any phrase-based analysis. In addition, these methods utilize synonymy to identify concepts and only explore hypernymy to calculate concept frequencies, without considering other semantic relationships such as hyponymy. To address these issues, we combine detection of noun phrases with the use of WordNet as background knowledge to explore better ways of representing documents semantically for clustering. First, based on noun phrases as well as single-term analysis, we exploit different document representation methods to analyze the effectiveness of hypernymy, hyponymy, holonymy, and meronymy. Second, we choose the most effective method and compare it with the WordNet-based clustering method proposed by others. The experimental results show the effectiveness of semantic relationships for clustering are (from highest to lowest): hypernymy, hyponymy, meronymy, and holonymy. Moreover, we found that noun phrase analysis improves the WordNet-based clustering method.}},
    address = {New York, NY, USA},
    author = {Zheng, Hai T. and Kang, Bo Y. and Kim, Hong G.},
    citeulike-article-id = {7163293},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1531037},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.ins.2009.02.019},
    doi = {10.1016/j.ins.2009.02.019},
    issn = {0020-0255},
    journal = {Inf. Sci.},
    keywords = {clustering, notavailable, notread},
    month = jun,
    number = {13},
    pages = {2249--2262},
    posted-at = {2015-04-24 17:30:05},
    priority = {1},
    publisher = {Elsevier Science Inc.},
    title = {{Exploiting Noun Phrases and Semantic Relationships for Text Document Clustering}},
    url = {http://dx.doi.org/10.1016/j.ins.2009.02.019},
    volume = {179},
    year = {2009}
}

@inproceedings{ClusterRefinementModelSelect@2002@Liu,
    abstract = {{In this paper, we propose a document clustering method that strives to achieve: (1) a high accuracy of document clustering, and (2) the capability of estimating the number of clusters in the document corpus (i.e. the model selection capability). To accurately cluster the given document corpus, we employ a richer feature set to represent each document, and use the Gaussian Mixture Model (GMM) together with the Expectation-Maximization (EM) algorithm to conduct an initial document clustering. From this initial result, we identify a set of discriminative featuresfor each cluster, and refine the initially obtained document clusters by voting on the cluster label of each document using this discriminative feature set. This self-refinement process of discriminative feature identification and cluster label voting is iteratively applied until the convergence of document clusters. On the other hand, the model selection capability is achieved by introducing randomness in the cluster initialization stage, and then discovering a value C for the number of clusters N by which running the document clustering process for a fixed number of times yields sufficiently similar results. Performance evaluations exhibit clear superiority of the proposed method with its improved document clustering and model selection accuracies. The evaluations also demonstrate how each feature as well as the cluster refinement process contribute to the document clustering accuracy.}},
    address = {New York, NY, USA},
    author = {Liu, Xin and Gong, Yihong and Xu, Wei and Zhu, Shenghuo},
    booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    citeulike-article-id = {104234},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=564411},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/564376.564411},
    doi = {10.1145/564376.564411},
    isbn = {1-58113-561-0},
    keywords = {clustering, notread},
    location = {Tampere, Finland},
    pages = {191--198},
    posted-at = {2015-04-24 17:22:14},
    priority = {5},
    publisher = {ACM},
    series = {SIGIR '02},
    title = {{Document Clustering with Cluster Refinement and Model Selection Capabilities}},
    url = {http://dx.doi.org/10.1145/564376.564411},
    year = {2002}
}

@article{WordNetAndFuzzyAssociation@2010@Chen,
    abstract = {{With the rapid growth of text documents, document clustering has become one of the main techniques for organizing large amount of documents into a small number of meaningful clusters. However, there still exist several challenges for document clustering, such as high dimensionality, scalability, accuracy, meaningful cluster labels, overlapping clusters, and extracting semantics from texts. In order to improve the quality of document clustering results, we propose an effective Fuzzy-based Multi-label Document Clustering (FMDC) approach that integrates fuzzy association rule mining with an existing ontology WordNet to alleviate these problems. In our approach, the key terms will be extracted from the document set, and the initial representation of all documents is further enriched by using hypernyms of WordNet in order to exploit the semantic relations between terms. Then, a fuzzy association rule mining algorithm for texts is employed to discover a set of highly-related fuzzy frequent itemsets, which contain key terms to be regarded as the labels of the candidate clusters. Finally, each document is dispatched into more than one target cluster by referring to these candidate clusters, and then the highly similar target clusters are merged. We conducted experiments to evaluate the performance based on Classic, Re0, R8, and WebKB datasets. The experimental results proved that our approach outperforms the influential document clustering methods with higher accuracy. Therefore, our approach not only provides more general and meaningful labels for documents, but also effectively generates overlapping clusters.}},
    address = {Amsterdam, The Netherlands, The Netherlands},
    author = {Chen, Chun L. and Tseng, Frank S. C. and Liang, Tyne},
    citeulike-article-id = {7839529},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1869133.1869215},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.datak.2010.08.003},
    day = {31},
    doi = {10.1016/j.datak.2010.08.003},
    issn = {0169-023X},
    journal = {Data Knowl. Eng.},
    keywords = {clustering, notread, wordnet},
    month = nov,
    number = {11},
    pages = {1208--1226},
    posted-at = {2015-04-24 17:10:53},
    priority = {5},
    publisher = {Elsevier Science Publishers B. V.},
    title = {{Editorial: An Integration of WordNet and Fuzzy Association Rule Mining for Multi-label Document Clustering}},
    url = {http://dx.doi.org/10.1016/j.datak.2010.08.003},
    volume = {69},
    year = {2010}
}

@article{ColumbiaMultiSumWang2011,
    abstract = {{Document understanding techniques such as document clustering and multidocument summarization have been receiving much attention recently. Current document clustering methods usually represent the given collection of documents as a document-term matrix and then conduct the clustering process. Although many of these clustering methods can group the documents effectively, it is still hard for people to capture the meaning of the documents since there is no satisfactory interpretation for each document cluster. A straightforward solution is to first cluster the documents and then summarize each document cluster using summarization methods. However, most of the current summarization methods are solely based on the sentence-term matrix and ignore the context dependence of the sentences. As a result, the generated summaries lack guidance from the document clusters. In this article, we propose a new language model to simultaneously cluster and summarize documents by making use of both the document-term and sentence-term matrices. By utilizing the mutual influence of document clustering and summarization, our method makes; (1) a better document clustering method with more meaningful interpretation; and (2) an effective document summarization method with guidance from document clustering. Experimental results on various document datasets show the effectiveness of our proposed method and the high interpretability of the generated summaries.}},
    address = {New York, NY, USA},
    author = {Wang, Dingding and Zhu, Shenghuo and Li, Tao and Chi, Yun and Gong, Yihong},
    citeulike-article-id = {13591131},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1993077.1993078},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1993077.1993078},
    doi = {10.1145/1993077.1993078},
    issn = {1556-4681},
    journal = {ACM Trans. Knowl. Discov. Data},
    keywords = {clustering},
    month = aug,
    number = {3},
    posted-at = {2015-04-24 16:28:11},
    priority = {5},
    publisher = {ACM},
    title = {{Integrating Document Clustering and Multidocument Summarization}},
    url = {http://dx.doi.org/10.1145/1993077.1993078},
    volume = {5},
    year = {2011}
}

@misc{SumEvaluation2001,
    abstract = {{This paper provides an overview of different methods
for evaluating automatic summarization systems.
The challenges in evaluating summaries are characterized.
Both intrinsic and extrinsic approaches are
discussed. Methods for assessing informativeness and
coherence are described. The advantages and disadvantages

of specific methods are assessed, along with
criteria for choosing among them. The paper concludes
with some suggestions for future directions.}},
    author = {Mani, I.},
    citeulike-article-id = {390651},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.2078},
    keywords = {summarization},
    posted-at = {2015-11-24 17:11:52},
    priority = {3},
    title = {{Summarization evaluation: An overview}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.2078},
    year = {2001}
}

@inproceedings{Carrot2Search2003,
    author = {Stefanowski, Jerzy and Weiss, Dawid},
    booktitle = {Web Intelligence, First International Atlantic Web Intelligence Conference, {AWIC} 2003, Madrid, Spain, May 5-6, 2003, Proceedings},
    citeulike-article-id = {13845644},
    keywords = {clustering, engine},
    pages = {240--249},
    posted-at = {2015-11-25 13:42:10},
    priority = {2},
    title = {{Carrot and Language Properties in Web Search Results Clustering}},
    year = {2003}
}

