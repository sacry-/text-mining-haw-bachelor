\epigraph{\emph{
  ``An algorithm must be seen to be believed.''
}}{ Donald Knuth }

In this section we evaluate some strategies for clustering. At first we will show how a single day of news events are clustered. Then clustering news events over multiple days. Due to time constraints it was not possible to elaborately test multiple days clustering. It is a notoriously difficult topic that greatly relies on the data and time. Originally it was intended to use a scraped dataset over several month, categorized by date, with as much meta data as possible. As this is at the barrier of legallity, a smaller standardized dataset namely the BBC dataset by \cite{BBCData2006} was used. In the original dataset it was possible to assume that the data was ordered and sorted by day. Several thousand articles could then be easily clustered on a daily basis and enhanced over the course of several months. Instead we are focusing on how the BBC dataset can be clustered and evaluated. Multiple day clustering will be depicted, but not in any form evaluated.\\

The BBC dataset had a few limitations in the sense that dates were not provided and meta data not provided by the authors. As such we treat the data as a a bulk of events occuring in the timeframe of 2004-2005. Fortunately unlike real world data the articles are tagged with precise categorical labels, so external evaluation measures were an option. There were limitations in time to find a more adequate set of articles that were consecutively sorted by day in a broader category such as ``world news''.\\

The clustering was done in several steps, similar to the Columbia Newsblaster system.\cite[Columbia stuff]{Nothing} A hierarchical clustering approach by first clustering events into categories like politics, business or entertainment and second clustering the categories into distinct events. The classification in the first step garantuees some broader topical cohesion. In the BBC dataset a purity of up to 94\% could be achieved on the test set. This is unusually high, but is probably due to the normalized dataset that represents the classes exceptionally well. The real world dataset could go as high as 45\% purity and less. \cite[some other evaluation here]{Nothing} The purity on the BBC dataset varies broadly between 79\% and 94\%. This variability results in overlapping topics where the majority of documents in a cluster are in the same class. Each of the resulting clusters resembling the underlying classes are reclustered. A clustering algorithm that optionally does not take any constraints on the amount of clusters is used. In this second step clusters represent news events on several topics per cluster.\\

Some final words on the measurement of the two steps. In the first step it is easy to evaluate how well documents were assigned to their respective classes. In the second step however evaluation is not that simple. At this point, we are interested in document similarity based on content and not class labels. The results of the second clustering are informal and not measured.

\section{Single day clustering}

Single day clustering is done by focussing on one specific date. The hypothesis is, as long as there are no major news headlines, documents will have a high variance in vector space. Thusly there will be more clusters. The major news headlines is important, because, during the original dataset scraping there were ground breaking events, that literally absored almost all articles into single clusters. That was expected because roughly 90\% of the articles were about the same news event. The angle of the different articles covering the event varied greatly enough that topics that had no underlying connection to the event were clustered into as well.\\

In the following the two steps are refined. First we want to classify news events by classes, in case of the BBC dataset: politics, business, entertainment, tech and sports. For this it is possible to use algorithms with a hard number of final clusters to classify documents into the classes. Alternatively one could use a supervised classifier, which in practice, is much more accurate. Second the resulting clusters, now assigned by news category are clustered again. This time the clustering algorithm has the constraint of automatically detecting the cluster numbers. For this a hierarchical algorithm with soft thresholds was chosen like BIRCH or Ward linkage. The resulting clusters resemble news events dealing with a similar topic.

  \subsection{Implementation}
  The implementation of a more general clustering algorithm in the text domain can vary drastically. A general scheme is given by algorithm \ref{general_clustering}.

    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \caption{General clustering}\label{general_clustering}
      \Function{general\_cluster}{$X_{train},y_{train},\kappa,\alpha,d$}
        \State $X_{norm},y_{norm} = normalize_{text}(X_{train},y_{train})$
        \State $X_{project},y_{project} = project(X_{norm},y_{norm})$
        \State $X_{trans},y_{trans} = transform(X_{project},y_{project})$

        \State $X_{lsi} = lsa(X_{trans}, topics=\alpha)$
        \State $X_{dim} = reduce\_dimensions(X_{lsi}, dimensions=d)$
        \State $X_{sim} = similarity(X_{dim})$
        \State $X_{scaled} = scale(X_{sim})$
        \State $model,centroids,labels,k = cluster(X_{scaled}, clusters=\kappa)$

        \State \Return $model,centroids,labels,k$
      \EndFunction
    \end{algorithmic}
    \end{algorithm}

  The general implementation \ref{general_clustering} is separated into several steps. As input the function gets a training set $X_{train}$ and a label set $y_{train}$. $\kappa$ is an optional parameter for the total amount of cluster centers. $\alpha$ a parameter that sets a desired size of topics for models like \lsa{}. And a dimension $d$ to reduce $X$ to for visualization purposes by e.g. \pca{}. In general we assume that the data $X_{train}$ is of any textual sort, the preprocessing needs to do the necessary steps that fits the function. Moreover it is of utmost importance to think in terms of vectorized code. Any procedures gradually transform a document into the vector space. After this all rules by linear algebra hold.

    \begin{enumerate}
      \item \emph{normalize} takes $X_{train}$ and $y_{train}$ to remove special characters, stopwords, numbers. Lower casing any words and removing non english characters or sentences. 
      \item \emph{project} is any kind of projection strategy via \wordnet{}, noun phrases or named entities, lowering word sense disambiguation and dimensions. This step deals with the initial seed of knowledge that is used throughout the algorithm.
      \item \emph{transform} takes in the projected data and transforms it via TF-IDF, word pruning, as well as ngram enhancement.
      \item \emph{lsa} is an optional step that transforms the resulting data to dense low dimensional matrices, keeping as much variance as possible while reducing noise. In this case \lsa{} was used but \lda{} or \hdp{} would be an option as well. 
      \item \emph{reduce\_dimensions} is an alternative step that reduces the dimension of the data to 2d or 3d plotting clusters. This is typically done by \pca{}.
      \item \emph{similarity} transforms document x term vectors to document to document similarity by cosine or euclidean distance measures.
      \item \emph{scale} is a second normalization step, scaling the data by variance and average.
      \item \emph{cluster} finally takes the matrix $X_{scaled}$ and clusters based on a hard constrained clustering number. If not supplied the clustering algorithm needs to predict a cluster number automatically. This results in the actual cluster amount $k$. The resulting trained clustering $model$. The $centroids$ in case they are actively calculated e.g. by \emph{K-Means}. And finally the $labels$, the assignment from a document to a corresponding cluster.
    \end{enumerate}

  The implementation is a rough scetch of a real clustering scheme. In reality most of the beforementioned steps can be parameterized or constrained by addtional arguments. Either a configuration object is specified or an extensive API used to create clustering scripts. The API used was $scikit-learn$ by \cite{ScikitLearn} and is part of the huge scientific computing libraries written in Python, such as $numpy$, $scipy$, $pandas$ etc. From this we can generalize for a clustering scheme over a single day.

    \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \caption{Single day clustering}\label{single_day_clustering}
        \State $X_{train},y_{train} = get\_data(timestamp)$
        \State $model,centroids,labels,k = general\_cluster(X_{train},y_{train},\kappa,\alpha)$
        \State $X_{assigned} = get\_assigned\_documents(X_{train},labels)$
        \State $news\_labels = []$
        \For{$c \in X_{assigned}$}
          \State $model',centroids',labels',k' = general\_cluster(c,nil,\alpha)$
          \State $assign(news\_labels, X_{train}, labels')$
        \EndFor
        \State \Return $news\_labels$
    \end{algorithmic}
    \end{algorithm}

  The implementation \ref{single_day_clustering} is a rough scetch of what the \emph{News-Clusty} system does. First get the data by a timestamp e.g. ``20150701'', cluster the data by a fixed $\kappa$ and then use the resulting cluster assignments $X_{assigned}$ to group news events. Note that $general\_cluster$ in the second run has a $nil$ argument, because we do not want to constrain by cluster amount. The clustering now depends entirely on the variation of the intermediate procedures of $general\_cluster$ and is now an optimization problem. 

  \subsection{Evaluation}
  Running several different strategies against each other we can find that several ways lead to a good result in the categorial clustering. The second clustering is not of utmost importance in the evaluation approach but is depicted later on. We can vary several parameters. To narrow down the combinatoric explosion in parameter estimation some assumptions can be made. During the time of the experiments, several parameters were optimized resulting in the following variations.

    \begin{enumerate}
      \item Limiting the strategies to simple word tokens, noun phrases and \wordnet{} first hypernym as well as lemmatization.

      \item \emph{Transforming} by \emph{TF-IDF} with a 80\% maximum threshold and a hard count $3$ for minimum threshold, excluding ngrams.

      \item \emph{topic modelling}, whether \lsa{} or \lda{} should be used or not. The topic size is supervisedly adjusted.

      \item Setting the \emph{distance measure} to \emph{cosine}.

      \item \emph{clustering algorithm} constrains, such as density thresholds and maximum \emph{HAC} depths. Classification is constrained by $k=5$ for the categories of the BBC dataset.
    \end{enumerate}

  Comparing the different strategies with \lsa{} enabled for the classification task comes down to 

    Image comparison

  without topic models

    Image comparison

  Resulting in..

  Second clustering...

    Notable clusters figure
    visualization of another clustering

\section{Multiple days clustering}
  
  multiple days, bulk in,
  continuous on predefined clusters
  event based model picture

