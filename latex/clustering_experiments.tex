\epigraph{\emph{
  ``An algorithm must be seen to be believed.''
}}{ Donald Knuth }

In this section we evaluate some strategies for clustering. At first we will show how a single day of news events are clustered. Then clustering news events over multiple days. Due to time constraints it was not possible to elaborately test multiple days clustering. It is a notoriously difficult topic that greatly relies on the data and time. Originally it was intended to use a scraped dataset over several month, categorized by date, with as much meta data as possible. As this is at the barrier of legallity, a smaller standardized dataset namely the BBC dataset by \cite{BBCData2006} was used. In the original dataset it was possible to assume that the data was ordered and sorted by day. Several thousand articles could then be easily clustered on a daily basis and enhanced over the course of several months. Instead we are focusing on how the BBC dataset can be clustered and evaluated. Multiple day clustering will be depicted, but not in any form evaluated.\\

The BBC dataset had a few limitations in the sense that dates were not provided and meta data not provided by the authors. As such we treat the data as a a bulk of events occuring in the timeframe of 2004-2005. Fortunately unlike real world data the articles are tagged with precise categorical labels, so external evaluation measures were an option. There were limitations in time to find a more adequate set of articles that were consecutively sorted by day in a broader category such as ``world news''.\\

The clustering was done in several steps, similar to the Columbia Newsblaster system.\cite{NewsBlaster2002} A hierarchical clustering approach by first clustering events into categories like politics, business or entertainment and second clustering the categories into distinct events. The classification in the first step garantuees some broader topical cohesion. In the BBC dataset a purity of up to 94\% could be achieved on the test set. This is unusually high, but is probably due to the normalized dataset that represents the classes exceptionally well. The real world dataset could go as high as 45\% purity and less. The purity on the BBC dataset varies broadly between 79\% and 94\%. This variability results in overlapping topics where the majority of documents in a cluster are in the same class. Each of the resulting clusters resembling the underlying classes are reclustered. A clustering algorithm that optionally does not take any constraints on the amount of clusters is used. In this second step clusters represent news events on several topics per cluster.\\

Some final words on the measurement of the two steps. In the first step it is easy to evaluate how well documents were assigned to their respective classes. In the second step however evaluation is not that simple. At this point, we are interested in document similarity based on content and not class labels. The results of the second clustering are informal and not measured.

\section{Single day clustering}

Single day clustering is done by focussing on one specific date. The hypothesis is, as long as there are no major news headlines, documents will have a high variance in vector space. Thusly there will be more clusters. The major news headlines is important, because, during the original dataset scraping there were ground breaking events, that literally absored almost all articles into single clusters. That was expected because roughly 90\% of the articles were about the same news event. The angle of the different articles covering the event varied greatly enough that topics that had no underlying connection to the event were clustered into as well.\\

In the following the two steps are refined. First we want to classify news events by classes, in case of the BBC dataset: politics, business, entertainment, tech and sports. For this it is possible to use algorithms with a hard number of final clusters to classify documents into the classes. Alternatively one could use a supervised classifier, which in practice, is much more accurate. See \cite{LearningMultiLabelKmeans2013} for a real world example learning labels in comparison of \emph{K-Means} and a \emph{supervised classifier}. Second the resulting clusters, now assigned by news category are clustered again. This time the clustering algorithm has the constraint of automatically detecting the cluster numbers. For this a hierarchical algorithm with soft thresholds was chosen like BIRCH or Ward linkage. The resulting clusters resemble news events dealing with a similar topic. 

\section{Implementation}
The implementation of a more general clustering algorithm in the text domain can vary drastically. A general scheme is given by algorithm \ref{general_clustering}.

  \begin{algorithm}[H]
  \begin{algorithmic}[1]
    \caption{General clustering}\label{general_clustering}
    \Function{general\_cluster}{$X_{train},y_{train},\kappa,\alpha,d$}
      \State $X_{norm},y_{norm} = normalize_{text}(X_{train},y_{train})$
      \State $X_{project},y_{project} = project(X_{norm},y_{norm})$
      \State $X_{trans},y_{trans} = transform(X_{project},y_{project})$

      \State $X_{lsi} = lsa(X_{trans}, topics=\alpha)$
      \State $X_{dim} = reduce\_dimensions(X_{lsi}, dimensions=d)$
      \State $X_{sim} = similarity(X_{dim})$
      \State $X_{scaled} = scale(X_{sim})$
      \State $model,centroids,labels,k = cluster(X_{scaled}, clusters=\kappa)$

      \State \Return $model,centroids,labels,k$
    \EndFunction
  \end{algorithmic}
  \end{algorithm}

The general implementation \ref{general_clustering} is separated into several steps. As input the function gets a training set $X_{train}$ and a label set $y_{train}$. $\kappa$ is an optional parameter for the total amount of cluster centers. $\alpha$ a parameter that sets a desired size of topics for models like \lsa{}. And a dimension $d$ to reduce $X$ to for visualization purposes by e.g. \pca{}. In general we assume that the data $X_{train}$ is of any textual sort, the preprocessing needs to do the necessary steps that fits the function. Moreover it is of utmost importance to think in terms of vectorized code. Any procedures gradually transform a document into the vector space. After this all rules by linear algebra hold.

  \begin{enumerate}
    \item \emph{normalize} takes $X_{train}$ and $y_{train}$ to remove special characters, stopwords, numbers. Lower casing any words and removing non english characters or sentences. 
    \item \emph{project} is any kind of projection strategy via \wordnet{}, noun phrases or named entities, lowering word sense disambiguation and dimensions. This step deals with the initial seed of knowledge that is used throughout the algorithm.
    \item \emph{transform} takes in the projected data and transforms it via TF-IDF, word pruning, as well as ngram enhancement.
    \item \emph{lsa} is an optional step that transforms the resulting data to dense low dimensional matrices, keeping as much variance as possible while reducing noise. In this case \lsa{} was used but \lda{} or \hdp{} would be an option as well. 
    \item \emph{reduce\_dimensions} is an alternative step that reduces the dimension of the data to 2d or 3d plotting clusters. This is typically done by \pca{}.
    \item \emph{similarity} transforms document x term vectors to document to document similarity by cosine or euclidean distance measures.
    \item \emph{scale} is a second normalization step, scaling the data by variance and average.
    \item \emph{cluster} finally takes the matrix $X_{scaled}$ and clusters based on a hard constrained clustering number. If not supplied the clustering algorithm needs to predict a cluster number automatically. This results in the actual cluster amount $k$. The resulting trained clustering $model$. The $centroids$ in case they are actively calculated e.g. by \emph{K-Means}. And finally the $labels$, the assignment from a document to a corresponding cluster.
  \end{enumerate}

The implementation is a rough scetch of a real clustering scheme. In reality most of the beforementioned steps can be parameterized or constrained by addtional arguments. Either a configuration object is specified or an extensive API used to create clustering scripts. The API used was $scikit-learn$ by \cite{ScikitLearn} and is part of the huge scientific computing libraries written in Python, such as $numpy$, $scipy$, $pandas$ etc. From this we can generalize for a clustering scheme over a single day.

  \begin{algorithm}[H]
  \begin{algorithmic}[1]
    \caption{Single day clustering}\label{single_day_clustering}
      \State $X_{train},y_{train} = get\_data(timestamp)$
      \State $model,centroids,labels,k = general\_cluster(X_{train},y_{train},\kappa,\alpha)$
      \State $X_{assigned} = get\_assigned\_documents(X_{train},labels)$
      \State $news\_labels = []$
      \For{$c \in X_{assigned}$}
        \State $model',centroids',labels',k' = general\_cluster(c,nil,\alpha)$
        \State $assign(news\_labels, X_{train}, labels')$
      \EndFor
      \State \Return $news\_labels$
  \end{algorithmic}
  \end{algorithm}

The implementation \ref{single_day_clustering} is a rough scetch of what the \emph{News-Clusty} system does. First get the data by a timestamp e.g. ``20150701'', cluster the data by a fixed $\kappa$ and then use the resulting cluster assignments $X_{assigned}$ to group news events. Note that $general\_cluster$ in the second run has a $nil$ argument, because we do not want to constrain by cluster amount. The clustering now depends entirely on the variation of the intermediate procedures of $general\_cluster$ and is now an optimization problem. 

\section{Evaluation}
In the evaluation section the beforementioned measuring strategies are used. Several different strategies are weighted against each other. It was found, that there are several different ways with a good performance in the categorial clustering approach. The second clustering is not of utmost importance in the evaluation but is depicted later on. A major problem is, that several parts of the proposed algorithm has parameters that need to be estimated. To narrow down this combinatoric explosion of parameters, several assumptions can be made.

  \begin{enumerate}
    \item Limiting the strategies to simple word tokens and their document titles except stopwords, noun phrases and ner tags as well as \wordnet{} first hypernym and \wordnet{}lemmatization.

    \item \emph{Transforming} by \emph{TF-IDF} with a 80\% maximum threshold and a hard count $3$ for minimum threshold, excluding ngrams.

    \item \emph{topic modelling}, whether \lsa{} or \lda{} should be used or not. The topic size is supervisedly adjusted.

    \item Setting the \emph{distance measure} to \emph{cosine}.

    \item \emph{clustering algorithm} constrains, such as density thresholds and maximum \emph{HAC} depths. Classification is constrained by $k=5$ for the categories of the BBC dataset.
  \end{enumerate}

A comparisson of the different strategies with \lsa{} enabled, for the classification task, is shown in table \ref{comparison_single_with_lsa}. Note that the \emph{purity}, \emph{v-measure} and \emph{silhouette score} is averaged over 5 consecutive runs.

  \begin{table}\label{comparison_single_with_lsa}
    \begin{tabular}{ c | c | c | c }
      Strategy    & v-measure & purity  & silhouette \\ \hline
      Word tokens & 0.812     & 0.926   & 0.118      \\
      Syntactic   & 0.795     & 0.916     & 0.109 \\
      Word + noun tokens & 0.826   & 0.934     & 0.118 \\
      \wordnet{} $hypernyms_{first}$ & 0.710   & 0.884     & 0.068 \\
      \wordnet{} lemmatization   & 0.726   & 0.895     & 0.069 \\
    \end{tabular}
    \caption{Comparison of feature selection with \lsa{}}
  \end{table}

Concluding from this, word tokens in combination with noun phrases lead to the highest v-measure. While other strategies perform still well enough, the chance for the best classification by clustering stems from purely syntactic means, e.g. word tokens and noun phrases. With \wordnet{} lemmatization we have a highly effective approach. That is, from the original 12000 feature dimensions we projected to 6000. Except for lemmatization, \wordnet{} approaches performed worse than the mentioned strategies. This is not generally the case as can be seen in \cite{TopicClassificationReuters2002} for an example on the Reuters Corpus with \wordnet{} using a multinominal Naive-Bayes classifier.\\
We can see in table \ref{comparison_single_without_lsa}, that the same measurement without \lsa{} performs in almost all instances worse. Note that \lda{} was explicitly excluded from the possible topic modelling techniques as it promotes a poor pre clustering step. This is due to the fact that \lda{} maps several topics to documents, resulting in documents having assignments to many topics. This is a desired property for the second clustering run.

  \begin{table}\label{comparison_single_without_lsa}
    \begin{tabular}{ c | c | c | c }
      Strategy    & v-measure & purity  & silhouette \\ \hline
      Word tokens & 0.667     & 0.831   & 0.073      \\
      Syntactic   & 0.634     & 0.794     & 0.067 \\
      Word + noun tokens & 0.666   & 0.831     & 0.074 \\
      \wordnet{} $hypernyms_{first}$ & 0.585   & 0.784     & 0.048 \\
      \wordnet{} lemmatization   & 0.585   & 0.784     & 0.048 \\
    \end{tabular}
    \caption{Comparison of feature selection without \lsa{}}
  \end{table}

The discrepancy between v-measure, purity and silouhette is expected. v-measure takes into account that several clusters contain an amount of labels that are not dominant. The higher the variance of spreading labels, even if their occurence count is low, leads to a penalty in v-measure. Purity on the other hand compares how many dominant labels are in one cluster and penalizes for each non dominant label. It does not account for the variety of falsely assigned labels. The silhouette score is extraordinary low due to the high dimensionality. It can not be taken in absolutes but rather in relative terms between different strategies. The higher the score the higher the cohesion between topic segments. In total, the silouhette score is better when the purity and v-measure is relatively higher.

\section{Experimental}
While clustering for classification is a very good first step to preselect documents into isolated clusters, the requirements change in the second run. The results do not need to be constrained by cluster amount. Selection of similar news events return smaller topical clusters. It is difficult to show that a connection between different documents can be made when no information about time are present. If a document about ``taxes in the us'' occured at a specific day and a second document about ``Obama raises taxes'' at the next day occured, we could conclude that it is a current topic in the news that are related. Without knowing when a document was published, it is not possible to assume that two documents about taxes, are about the same event or piece of information. The second event could have happened 9 months later or earlier. The content might be connected but the events are not. There is no sense in trying to show how two consecutive days would relate to each other. That is why visual representations of topic and word proportions in clusters will be shown, instead of timeline data that relates. Two additional clusterings with BIRCH and \lda{} are proposed.\\

First let us demonstrate the generative model. In this instance \lda{} has shown promising informal results. \hdp{} would be an optimal choice due to the automatic topic amount detection, but was left out as it was too complex to adapt. In figure \ref{business_topics} we see a clustering by \lda{} over the business cluster.

  \begin{table}[h!]\label{business_topics}
    \[
      \kbordermatrix{%
        topics\textbackslash{}words & w_1  & w_2  & w_3    & w_4  & w_5   \\
        t_1 & strong  & growth     & manufacturing & bank      & economy  \\
        t_2 & city    & boeing     & year          & company   & industry \\
        t_3 & year    & india      & germany       & market    & growth   \\
        t_4 & tobacco & government & court         & bn        & case     \\
        t_5 & sales   & car        & gm            & bmw       & year     \\
        t_6 & economy & spending   & growth        & recession & japan    \\
      }
    \]
    \caption{"Business topic proportions"}
  \end{table}

Typical words such as ``growth'', ``manufacturing'' or ``bank'' resemble this. A second example of a word to topic proportion can be seen in figure \ref{political_topics} for political topics.

  \begin{table}[h!]\label{politic_topics}
    \[
      \kbordermatrix{%
        topics\textbackslash{}words & w_1  & w_2  & w_3    & w_4  & w_5   \\
        t_1 & election    & new     & blair     &  minister   & tax       \\
        t_2 & government  & home    & law       & people      & police    \\
        t_3 & government  & defence & uk        & guantanamo  & evidence  \\
        t_4 & million     & year    & education & party       & marketing \\
        t_5 & uk          & card    & community & troops      & fraud     \\
        t_6 & access      & child   & parents   & years       & care      \\
      }
    \]
    \caption{"Political topic proportions"}
  \end{table}

As these word clouds have little value by measurement, they are easy to interpret. In reality both the political and business cluster had roughly 50 subclusters, only showing the first 6 and their respective top 5 words. Note that a topic from \lda{} is not exactly the same as centroids of a \emph{K-Means} algorithm. \lda{} topics relate to centroids as the \emph{EM} algorithm to \emph{K-Means}.\\

The second approach with the BIRCH algorithm yielded some interesting results as well. We can see in figure \ref{birch_clustering} that there are a lot of clusters of different sizes. There are much bigger clusters and smaller clusters. In news events we mainly want smaller coherent clusters. The centroids are the bigger points painted in a distinct color. Smaller points are documents assigned to a centroid in the same distinct color.

  \begin{figure}[h!]
    \centering
      \includegraphics[width=0.9\textwidth]{birch_clustering.png}
      \caption{"BIRCH clustering of a single day"}
      \label{birch_clustering}
  \end{figure}

The data at hand does not stem from the BBC dataset but from daily scraped newspapers. The papers shall not be named due to legal reasons. It is reasonable to believe that even less important events have multiple cluster points from different news sources. One can see that the resulting clusters are small and well formed. Some cluster centers are bigger than others. In those cases we can recluster them to further enhance the variance of clusters. The structures resemble that there are news that are covered several times a day and some that are not. Singleton clusters that are presented as x for one time events. Those outliers can be reclustered in a different run and would cover news like ``other''. The last figure \ref{mean_shift_clustering} is a clustering with the \emph{Mean Shift} algorithm. It is a partitonal algorithm working with density distributions, automatically infering the size of clusters by a density threshold.

  \begin{figure}[h!]
    \centering
      \includegraphics[width=0.9\textwidth]{mean_shift_clustering.png}
      \caption{"Mean Shift clustering of a single day"}
      \label{mean_shift_clustering}
  \end{figure} 

The clusters of the \emph{Mean Shift} are even smaller. The density threshold was carefully fine tuned over several runs. All visualizations were made of data containing roughly 350 documents from the category \emph{world} by two newspapers. Expecting at least 100 events covering different news. The final clusters visualizations are a result of several parameter estimations of the algorithms. All visualizations were done with the help of a Python API called \emph{matplotlib} by \cite{MatPlotHunter2007}.

\section{Multiple days clustering}
  Multiple days clustering was of the greatest interest while starting this thesis. It came to an understanding that it is inherently difficult to cluster and track documents over the course of several weeks. The time ran out to actually cluster multiple days in a row. Also, due to the problem of legallity, the BBC dataset came to the rescue. Multiple days clustering is an advancement on the single day clustering. See section \ref{sec:future_work} for future work.



