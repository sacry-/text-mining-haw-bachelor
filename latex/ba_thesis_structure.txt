
- Front Page
- Thesis Properties
- Abstract

Table of content:

1.) Introduction
  Why and relevance?
  What can be expected?
  What did I do?
  What did my work achieve?
  The Objective

  1.) Artifical Intelligence and Machine Learning
    1.) Small AI overview
    2.) Unsupervised vs Supervised Learning
    3.) Relatedness to Information Retrieval

  2.) Structure of the thesis
    what happens in chapter 2..n and how are they related?


2.) Basics
  1.) Definition of Clustering
    1.) Clustering types
      1.) Topic / Category of a document
      2.) Cluster a single document
      3.) Cluster several documents
      4.) Classification of clustering algorithm
    2.) Single day clustering
    3.) Continuous clustering

  2.) Vector Space Model (VSM)
    1.) Bag of words (document x term)
    2.) Tackling Sparsity (Hashing) 
    3.) Term frequencey - Inverse document frequency (TfIdf)
    4.) Similiarties and Measures
    5.) Enhancing the VSM
      1.) Singular Value Decomposition (SVD)
      2.) Latent Semantic Analysis (LSA)
      3.) Principal Component Analysis (PCA)

  3.) Probability and Topic models
    1.) Bayes Theorem / Inference / Conditional Probabilities
    2.) Multinominal Distributions
    3.) Probabilistic topic models
      Word and document proportionality, Gibbs Sampling, Dirichlet Distributions

  4.) Unsupervised clustering algorithms
    0.) A general clustering scheme and EM
    1.) Hierarchical / Agglomerative
      1.) Ward
      2.) Complete Linkage
      3.) Average Linkage 
      4.) Birch
    2.) Partitional
      1.) K-Means
      2.) Mean-Shift
    3.) Generative    
      1.) Non Negative Matrix Factorization (NMF)
      2.) Latent Dirichlet Allocation (LDA)
      3.) Hierarchical Dirichlet Process (HDP)
    4.) others
      Spectral, Density, Grid

  5.) Supervised (classification) strategies
    Small Overview: 
      (Multinominal/Gaussian) Naive Bayes (NB), 
      (multivariate) logistic/linear regression, 
      Neural Networks (ANN), 
      Support Vector Machines (SVM), 
      Random Forests (RF)

  6.) Cluster quality measures
    1.) Internal measure (without labels)
      Davies–Bouldin index
      Dunn index
      Silhouette coefficient
    2.) External measures (with labels)
      F-Measure
      Jaccard index

3.) A News summarization System (News Clusty)
  1.) Python and Libraries (Github Link - make public)
  2.) Databases (Persistence)
  3.) Components
    Facade
    Scraping
    Preprocessing
    Features
    Clustering
  4.) What is missing? Problems

4.) Documents and Features
  1.) Document domain
    1.) Newspapers
    2.) Metadata
    3.) Category knowledge
    4.) Sparsity
    5.) Special words
  2.) Selection process
  3.) Adding Semantics
    1.) Pos tags and noun phrases
    2.) Named Entities
    3.) Wikipedia
    4.) Wordnet

5.) Clustering with News Clusty
  1.) Single day clustering
    1.) Strategy 1 ( simple model )
      1.) Implementation
      2.) Measures
      3.) Interpretation
    2.) Strategy 2 ( generative models )
      1.) Implementation
      2.) Measures
      3.) Interpretation
    3.) Strategy 3 ( mixed complex model )
      1.) Implementation
      2.) Measures
      3.) Interpretation
  2.) Continuous clustering
    Open Question ?

6.) Results and discussion
  1.) Comparing all implementations
  2.) The Goods and the Bads
  3.) Conclusion

7.) Outlook
  1.) Summary
  2.) Further Reading / Related Work
  3.) Future Work
  4.) Final Words

- List of figures
- List of Tables
- Abbreviations

Chapters 1 - 7

- Appendix
- Glossary
- Bibliography
- Erklärung zur selbstständigen verfassten BA


-----------------

Quotes:

Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering.

— Andrew Ng

More data beats clever algorithms, but better data beats more data.

- Peter Norvig

Simple models and a lot of data trump more elaborate models based on less data.

- Peter Norvig

"If I have seen further it is by standing on the shoulders of giants."

- Isaac Newton

"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." 

- John Tukey

-----------------

